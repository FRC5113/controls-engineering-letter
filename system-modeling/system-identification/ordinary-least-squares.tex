\section{Ordinary least squares}

The parameter estimation in this chapter will be performed using ordinary least
squares (OLS). Let there be a linear equation of the form
$y = \beta_1 x_1 + \ldots + \beta_p x_p$. We want to find the constants
$\beta_1, \ldots, \beta_p$ that best fit a set of observations represented by
$x_{1,\ldots,n}$-$y$ tuples. Consider the overdetermined system of $n$ linear
equations
$\mat{y} = \mat{X}\bm{\beta}$ where
\begin{equation*}
  \mat{y} = \begin{bmatrix}
    y_1 \\
    \vdots \\
    y_n
  \end{bmatrix}
  \quad
  \mat{X} = \begin{bmatrix}
    x_{11} & \cdots & x_{1p} \\
    \vdots & \ddots & \vdots \\
    x_{n1} & \cdots & x_{np}
  \end{bmatrix}
  \quad
  \bm{\beta} = \begin{bmatrix}
    \beta_1 \\
    \vdots \\
    \beta_p
  \end{bmatrix}
\end{equation*}

each row corresponds to a datapoint, and $n > p$ (more datapoints than
unknowns). OLS is a type of linear least squares method that estimates the
unknown parameters $\bm{\beta}$ in a linear regression model
$\mat{y} = \mat{X}\bm{\beta} + \bm{\epsilon}$ where $\bm{\epsilon}$ is the error
in the observations $\mat{y}$. $\bm{\beta}$ is chosen by the method of least
squares: minimizing the sum of the squares of the difference between $\mat{y}$
(observations of the dependent variable) and $\mat{X}\bm{\beta}$ (predictions of
$\mat{y}$ using a linear function of the independent variable $\bm{\beta}$).

Geometrically, this is the sum of the squared distances, parallel to the y-axis,
between each value in $\mat{y}$ and the corresponding value in
$\mat{X}\bm{\beta}$. Smaller differences mean a better model fit.

To find the $\bm{\beta}$ that fits best, we can solve the following quadratic
minimization problem
\begin{equation*}
  \hat{\bm{\beta}} = \argmin_{\bm{\beta}} (\mat{y} - \mat{X}\bm{\beta})\T
    (\mat{y} - \mat{X}\bm{\beta})
\end{equation*}

$\argmin_{\bm{\beta}}$ means ``find the value of $\bm{\beta}$ that minimizes the
following function of $\bm{\beta}$". Given corollary \ref{cor:partial_ax_b},
take the partial derivative of the cost function with respect to $\bm{\beta}$
and set it equal to zero, then solve for $\hat{\bm{\beta}}$.
\begin{align}
  \mat{0} &= -2\mat{X}\T (\mat{y} - \mat{X}\hat{\bm{\beta}}) \nonumber \\
  \mat{0} &= \mat{X}\T (\mat{y} - \mat{X}\hat{\bm{\beta}}) \nonumber \\
  \mat{0} &= \mat{X}\T \mat{y} - \mat{X}\T\mat{X}\hat{\bm{\beta}} \nonumber \\
  \mat{X}\T\mat{X}\hat{\bm{\beta}} &= \mat{X}\T \mat{y} \nonumber \\
  \hat{\bm{\beta}} &= (\mat{X}\T\mat{X})^{-1} \mat{X}\T \mat{y} \label{eq:ols}
\end{align}

\subsection{Examples}

While this is a linear regression, we can fit nonlinear functions by making the
contents of $\mat{X}$ nonlinear functions of the independent variables. For
example, we can find the quadratic equation $y = ax^2 + bx + c$ that best fits
a set of $x$-$y$ tuples. Lets assume we have a set of observations as follows.
\begin{align*}
  y_1 &= ax_1^2 + bx_1 + c \\
  &\ \ \vdots \\
  y_n &= ax_n^2 + bx_n + c
  \intertext{We want to find $a$, $b$, and $c$, so let's factor those out.}
  \begin{bmatrix}
    y_1 \\
    \vdots \\
    y_n
  \end{bmatrix} &=
  \begin{bmatrix}
    x_1^2 & x_1 & 1 \\
    \vdots & \vdots & \vdots \\
    x_n^2 & x_n & 1
  \end{bmatrix}
  \begin{bmatrix}
    a \\
    b \\
    c
  \end{bmatrix}
\end{align*}

Plug these matrices into equation \eqref{eq:ols} to obtain the coefficients $a$,
$b$, and $c$.
\begin{equation*}
  \mat{y} = \begin{bmatrix}
    y_1 \\
    \vdots \\
    y_n
  \end{bmatrix}
  \quad
  \mat{X} = \begin{bmatrix}
    x_1^2 & x_1 & 1 \\
    \vdots & \vdots & \vdots \\
    x_n^2 & x_n & 1
  \end{bmatrix}
  \quad
  \bm{\beta} = \begin{bmatrix}
    a \\
    b \\
    c
  \end{bmatrix}
\end{equation*}
