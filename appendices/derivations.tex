\chapterimage{appendices.jpg}{Sunset in an airplane over New Mexico}

\chapter{Derivations}

\section{Transfer function in feedback}
\label{sec:deriv_tf_feedback}

Given the feedback network in figure \ref{fig:closed_loop_deriv}, find an
expression for $Y(s)$.
\begin{bookfigure}
  \begin{tikzpicture}[auto, >=latex']
    % Place the blocks
    \node [name=input] {$X(s)$};
    \node [sum, right=of input] (sum) {};
    \node [block, right=of sum] (G) {$G(s)$};
    \node [right=of G] (output) {$Y(s)$};
    \node [block, below=of G] (measurements) {$H(s)$};

    % Connect the nodes
    \draw [arrow] (input) -- node[pos=0.85] {$+$} (sum);
    \draw [arrow] (sum) -- node {$Z(s)$} (G);
    \draw [arrow] (G) -- node [name=y] {} (output);
    \draw [arrow] (y) |- (measurements);
    \draw [arrow] (measurements) -| node[pos=0.99, right] {$-$} (sum);
  \end{tikzpicture}

  \caption{Closed-loop block diagram}
  \label{fig:closed_loop_deriv}
\end{bookfigure}
\begin{align}
  Y(s) &= Z(s) G(s) \nonumber \\
  Z(s) &= X(s) - Y(s) H(s) \nonumber \\
  X(s) &= Z(s) + Y(s) H(s) \nonumber \\
  X(s) &= Z(s) + Z(s) G(s) H(s) \nonumber \\
  \frac{Y(s)}{X(s)} &= \frac{Z(s) G(s)}{Z(s) + Z(s) G(s) H(s)} \nonumber \\
  \frac{Y(s)}{X(s)} &= \frac{G(s)}{1 + G(s) H(s)}
\end{align}

A more general form is
\begin{equation}
  \frac{Y(s)}{X(s)} = \frac{G(s)}{1 \mp G(s) H(s)}
\end{equation}

where positive feedback uses the top sign and negative feedback uses the bottom
sign.

\section{Zero-order hold for state-space}
\label{sec:deriv_zoh_ss}

Starting with the continuous \gls{model}
\begin{equation*}
  \dot{\mat{x}}(t) = \mat{A}\mat{x}(t) + \mat{B}\mat{u}(t)
\end{equation*}

by premultiplying the \gls{model} by $e^{-\mat{A}t}$, we get
\begin{align*}
  e^{-\mat{A}t}\dot{\mat{x}}(t) &= e^{-\mat{A}t}\mat{A}\mat{x}(t) +
    e^{-\mat{A}t}\mat{B}\mat{u}(t) \\
  e^{-\mat{A}t}\dot{\mat{x}}(t) - e^{-\mat{A}t}\mat{A}\mat{x}(t) &=
    e^{-\mat{A}t}\mat{B}\mat{u}(t)
\end{align*}

The derivative of the matrix exponential is
\begin{equation*}
  \frac{d}{dt}e^{\mat{A}t} = \mat{A}e^{\mat{A}t} = e^{\mat{A}t}\mat{A}
\end{equation*}

so we recognize the previous equation as
\begin{equation*}
  \frac{d}{dt}\left(e^{-\mat{A}t}\mat{x}(t)\right) =
    e^{-\mat{A}t}\mat{B}\mat{u}(t)
\end{equation*}

By integrating this equation, we get
\begin{align*}
  e^{-\mat{A}t}\mat{x}(t) - e^0\mat{x}(0) &=
    \int_0^t e^{-\mat{A}\tau}\mat{B}\mat{u}(\tau) \,d\tau \\
  \mat{x}(t) &= e^{\mat{A}t}\mat{x}(0) +
    \int_0^t e^{\mat{A}(t - \tau)}\mat{B}\mat{u}(\tau) \,d\tau
\end{align*}

which is an analytical solution to the continuous \gls{model}. Now we want to
\glslink{discretization}{discretize} it.
\begin{align*}
  \mat{x}_k &\stackrel{def}{=} \mat{x}(kT) \\
  \mat{x}_k &= e^{\mat{A}kT}\mat{x}(0) +
    \int_0^{kT} e^{\mat{A}(kT - \tau)}\mat{B}\mat{u}(\tau) \,d\tau \\
  \mat{x}_{k+1} &= e^{\mat{A}(k + 1)T}\mat{x}(0) +
    \int_0^{(k + 1)T} e^{\mat{A}((k + 1)T - \tau)}\mat{B}\mat{u}(\tau) \,d\tau
    \\
  \mat{x}_{k+1} &= e^{\mat{A}(k + 1)T}\mat{x}(0) +
    \int_0^{kT} e^{\mat{A}((k + 1)T - \tau)}\mat{B}\mat{u}(\tau) \,d\tau +
    \int_{kT}^{(k + 1)T} e^{\mat{A}((k + 1)T - \tau)}\mat{B}\mat{u}(\tau)
    \,d\tau \\
  \mat{x}_{k+1} &= e^{\mat{A}(k + 1)T}\mat{x}(0) +
    \int_0^{kT} e^{\mat{A}((k + 1)T - \tau)}\mat{B}\mat{u}(\tau) \,d\tau +
    \int_{kT}^{(k + 1)T} e^{\mat{A}(kT + T - \tau)}\mat{B}\mat{u}(\tau) \,d\tau
    \\
  \mat{x}_{k+1} &= e^{\mat{A}T} \underbrace{\left(e^{\mat{A}kT}\mat{x}(0) +
    \int_0^{kT} e^{\mat{A}(kT - \tau)}\mat{B}\mat{u}(\tau)
    \,d\tau\right)}_{\mat{x}_k} +
    \int_{kT}^{(k + 1)T} e^{\mat{A}(kT + T - \tau)}\mat{B}\mat{u}(\tau) \,d\tau
\end{align*}

We assume that $\mat{u}$ is constant during each timestep, so it can be pulled
out of the integral.
\begin{equation*}
  \mat{x}_{k+1} = e^{\mat{A}T}\mat{x}_k +
    \left(\int_{kT}^{(k + 1)T} e^{\mat{A}(kT + T - \tau)} \,d\tau\right)
    \mat{B}\mat{u}_k
\end{equation*}

The second term can be simplified by substituting it with the function
$v(\tau) = kT + T - \tau$. Note that $d\tau = -dv$.
\begin{align*}
  \mat{x}_{k+1} &= e^{\mat{A}T}\mat{x}_k -
    \left(\int_{v(kT)}^{v((k + 1)T)} e^{\mat{A}v} \,dv\right)
    \mat{B}\mat{u}_k \\
  \mat{x}_{k+1} &= e^{\mat{A}T}\mat{x}_k -
    \left(\int_T^0 e^{\mat{A}v} \,dv\right) \mat{B}\mat{u}_k \\
  \mat{x}_{k+1} &= e^{\mat{A}T}\mat{x}_k +
    \left(\int_0^T e^{\mat{A}v} \,dv\right) \mat{B}\mat{u}_k \\
  \mat{x}_{k+1} &= e^{\mat{A}T}\mat{x}_k +
    \mat{A}^{-1}e^{\mat{A}v} \rvert_0^T \mat{B}\mat{u}_k \\
  \mat{x}_{k+1} &= e^{\mat{A}T}\mat{x}_k +
    \mat{A}^{-1}(e^{\mat{A}T} - e^{\mat{A}0}) \mat{B}\mat{u}_k \\
  \mat{x}_{k+1} &= e^{\mat{A}T}\mat{x}_k +
    \mat{A}^{-1}(e^{\mat{A}T} - \mat{I}) \mat{B}\mat{u}_k
\end{align*}

which is an exact solution to the \gls{discretization} problem.

\section{Kalman filter as Luenberger observer}
\label{sec:deriv_kalman_luenberger}

A Luenberger \gls{observer} is defined as
\begin{align}
  \hat{\mat{x}}_{k+1}^+ &= \mat{A}\hat{\mat{x}}_k^- + \mat{B}\mat{u}_k + \mat{L}
    (\mat{y}_k - \hat{\mat{y}}_k) \label{eq:luenberger1} \\
  \hat{\mat{y}}_k &= \mat{C} \hat{\mat{x}}_k^- \label{eq:luenberger2}
\end{align}

where a superscript of minus denotes \textit{a priori} and plus denotes
\textit{a posteriori} estimate. Combining equation \eqref{eq:luenberger1} and
equation \eqref{eq:luenberger2} gives
\begin{equation} \label{eq:luenberger}
  \hat{\mat{x}}_{k+1}^+ = \mat{A}\hat{\mat{x}}_k^- + \mat{B}\mat{u}_k + \mat{L}
    (\mat{y}_k - \mat{C}\hat{\mat{x}}_k^-)
\end{equation}

The following is a Kalman filter that considers the current update step and the
next predict step together rather than the current predict step and current
update step.
\begin{align}
  \text{Update step} \nonumber \\
  \mat{K}_k &= \mat{P}_k^- \mat{C}^T (\mat{C}\mat{P}_k^- \mat{C}^T +
    \mat{R})^{-1} \\
  \hat{\mat{x}}_k^+ &= \hat{\mat{x}}_k^- + \mat{K}_k(\mat{y}_k -
    \mat{C}\hat{\mat{x}}_k^-) \label{eq:post2_x} \\
  \mat{P}_k^+ &= (\mat{I} - \mat{K}_k\mat{C})\mat{P}_k^- \\
  \text{Predict step} \nonumber \\
  \hat{\mat{x}}_{k+1}^+ &= \mat{A}\hat{\mat{x}}_k^+ + \mat{B}\mat{u}_k
    \label{eq:pre2_x} \\
  \mat{P}_{k+1}^- &= \mat{A} \mat{P}_k^+ \mat{A}^T +
    \mat{\Gamma}\mat{Q}\mat{\Gamma}^T
\end{align}

Substitute equation \eqref{eq:post2_x} into equation \eqref{eq:pre2_x}.
\begin{align*}
  \hat{\mat{x}}_{k+1}^+ &= \mat{A}(\hat{\mat{x}}_k^- + \mat{K}_k(\mat{y}_k -
    \mat{C}\hat{\mat{x}}_k^-)) + \mat{B}\mat{u}_k \\
  \hat{\mat{x}}_{k+1}^+ &= \mat{A}\mat{x}_k^- + \mat{A}\mat{K}_k(\mat{y}_k -
    \mat{C}\hat{\mat{x}}_k^-) + \mat{B}\mat{u}_k \\
  \hat{\mat{x}}_{k+1}^+ &= \mat{A}\hat{\mat{x}}_k^- + \mat{B}\mat{u}_k +
    \mat{A}\mat{K}_k(\mat{y}_k - \mat{C}\hat{\mat{x}}_k^-)
\end{align*}

Let $\mat{L} = \mat{A} \mat{K}_k$.
\begin{equation}
  \hat{\mat{x}}_{k+1}^+ = \mat{A}\hat{\mat{x}}_k^- + \mat{B}\mat{u}_k + \mat{L}
    (\mat{y}_k - \mat{C}\hat{\mat{x}}_k^-)
\end{equation}

which matches equation \eqref{eq:luenberger}. Therefore, the eigenvalues of the
Kalman filter \gls{observer} can be obtained by
\begin{align}
  &\eig(\mat{A} - \mat{L}\mat{C}) \nonumber \\
  &\eig(\mat{A} - (\mat{A}\mat{K}_k)(\mat{C})) \nonumber \\
  &\eig(\mat{A}(\mat{I} - \mat{K}_k\mat{C}))
\end{align}

\subsection{Luenberger observer with separate prediction and update}
\label{subsec:deriv_luenberger_separate}

To run a Luenberger \gls{observer} with separate prediction and update steps,
substitute the relationship between the Luenberger \gls{observer} and Kalman
filter matrices derived above into the Kalman filter equations.

Appendix \ref{sec:deriv_kalman_luenberger} shows that
$\mat{L} = \mat{A}\mat{K}_k$. Since $\mat{L}$ and $\mat{A}$ are constant, one
must assume $\mat{K}_k$ has reached steady-state. Then,
$\mat{K} = \mat{A}^{-1}\mat{L}$. Substitute this into the Kalman filter update
equation.
\begin{align*}
  \hat{\mat{x}}_{k+1}^+ &= \hat{\mat{x}}_{k+1}^- + \mat{K}(\mat{y}_{k+1} -
    \mat{C}\hat{\mat{x}}_{k+1}^-) \\
  \hat{\mat{x}}_{k+1}^+ &= \hat{\mat{x}}_{k+1}^- + \mat{A}^{-1}\mat{L}
    (\mat{y}_{k+1} - \mat{C}\hat{\mat{x}}_{k+1}^-)
\end{align*}

Substitute in equation \eqref{eq:z_obsv_y}.
\begin{equation*}
  \hat{\mat{x}}_{k+1}^+ = \hat{\mat{x}}_{k+1}^- + \mat{A}^{-1}\mat{L}
    (\mat{y}_{k+1} - \hat{\mat{y}}_{k+1})
\end{equation*}

The predict step is the same as the Kalman filter's. Therefore, a Luenberger
\gls{observer} run with prediction and update steps is written as follows.
\begin{align}
  \text{Predict step} \nonumber \\
  \hat{\mat{x}}_{k+1}^- &= \mat{A}\hat{\mat{x}}_k^- + \mat{B}\mat{u}_k \\
  \text{Update step} \nonumber \\
  \hat{\mat{x}}_{k+1}^+ &= \hat{\mat{x}}_{k+1}^- + \mat{A}^{-1}\mat{L}
    (\mat{y}_{k+1} - \hat{\mat{y}}_{k+1}) \\
  \hat{\mat{y}}_{k+1} &= \mat{C} \hat{\mat{x}}_{k+1}^-
\end{align}

\section{Trapezoidal motion profile}
\label{sec:deriv_trapezoid_profile}
