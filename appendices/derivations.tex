\chapterimage{appendices.jpg}{Sunset in an airplane over New Mexico}

\chapter{Derivations}

\section{Transfer function in feedback}
\label{sec:deriv_tf_feedback}

Given the feedback network in figure \ref{fig:closed_loop_deriv}, find an
expression for $Y(s)$.

\begin{bookfigure}
  \begin{tikzpicture}[auto, >=latex']
    % Place the blocks
    \node [name=input] {$X(s)$};
    \node [sum, right=of input] (sum) {};
    \node [block, right=of sum] (G) {$G(s)$};
    \node [right=of G] (output) {$Y(s)$};
    \node [block, below=of G] (measurements) {$H(s)$};

    % Connect the nodes
    \draw [arrow] (input) -- node[pos=0.85] {$+$} (sum);
    \draw [arrow] (sum) -- node {$Z(s)$} (G);
    \draw [arrow] (G) -- node [name=y] {} (output);
    \draw [arrow] (y) |- (measurements);
    \draw [arrow] (measurements) -| node[pos=0.99, right] {$-$} (sum);
  \end{tikzpicture}

  \caption{Closed-loop block diagram}
  \label{fig:closed_loop_deriv}
\end{bookfigure}

\begin{align}
  Y(s) &= Z(s) G(s) \nonumber \\
  Z(s) &= X(s) - Y(s) H(s) \nonumber \\
  X(s) &= Z(s) + Y(s) H(s) \nonumber \\
  X(s) &= Z(s) + Z(s) G(s) H(s) \nonumber \\
  \frac{Y(s)}{X(s)} &= \frac{Z(s) G(s)}{Z(s) + Z(s) G(s) H(s)} \nonumber \\
  \frac{Y(s)}{X(s)} &= \frac{G(s)}{1 + G(s) H(s)}
\end{align}

A more general form is

\begin{equation}
  \frac{Y(s)}{X(s)} = \frac{G(s)}{1 \mp G(s) H(s)}
\end{equation}

where positive feedback uses the top sign and negative feedback uses the bottom
sign.

\section{Zero-order hold for state-space}
\label{sec:deriv_zoh_ss}

Starting with the continuous \gls{model}

\begin{equation*}
  \dot{\mtx{x}}(t) = \mtx{A}\mtx{x}(t) + \mtx{B}\mtx{u}(t)
\end{equation*}

by premultiplying the \gls{model} by $e^{-\mtx{A}t}$, we get

\begin{align*}
  e^{-\mtx{A}t}\dot{\mtx{x}}(t) &= e^{-\mtx{A}t}\mtx{A}\mtx{x}(t) +
    e^{-\mtx{A}t}\mtx{B}\mtx{u}(t) \\
  e^{-\mtx{A}t}\dot{\mtx{x}}(t) - e^{-\mtx{A}t}\mtx{A}\mtx{x}(t) &=
    e^{-\mtx{A}t}\mtx{B}\mtx{u}(t)
\end{align*}

The derivative of the matrix exponential is

\begin{equation*}
  \frac{d}{dt}e^{\mtx{A}t} = \mtx{A}e^{\mtx{A}t} = e^{\mtx{A}t}\mtx{A}
\end{equation*}

so we recognize the previous equation as

\begin{equation*}
  \frac{d}{dt}\left(e^{-\mtx{A}t}\mtx{x}(t)\right) =
    e^{-\mtx{A}t}\mtx{B}\mtx{u}(t)
\end{equation*}

By integrating this equation, we get

\begin{align*}
  e^{-\mtx{A}t}\mtx{x}(t) - e^0\mtx{x}(0) &=
    \int_0^t e^{-\mtx{A}\tau}\mtx{B}\mtx{u}(\tau) \,d\tau \\
  \mtx{x}(t) &= e^{\mtx{A}t}\mtx{x}(0) +
    \int_0^t e^{\mtx{A}(t - \tau)}\mtx{B}\mtx{u}(\tau) \,d\tau
\end{align*}

which is an analytical solution to the continuous \gls{model}. Now we want to
\glslink{discretization}{discretize} it.

\begin{align*}
  \mtx{x}_k &\stackrel{def}{=} \mtx{x}(kT) \\
  \mtx{x}_k &= e^{\mtx{A}kT}\mtx{x}(0) +
    \int_0^{kT} e^{\mtx{A}(kT - \tau)}\mtx{B}\mtx{u}(\tau) \,d\tau \\
  \mtx{x}_{k+1} &= e^{\mtx{A}(k + 1)T}\mtx{x}(0) +
    \int_0^{(k + 1)T} e^{\mtx{A}((k + 1)T - \tau)}\mtx{B}\mtx{u}(\tau) \,d\tau
    \\
  \mtx{x}_{k+1} &= e^{\mtx{A}(k + 1)T}\mtx{x}(0) +
    \int_0^{kT} e^{\mtx{A}((k + 1)T - \tau)}\mtx{B}\mtx{u}(\tau) \,d\tau +
    \int_{kT}^{(k + 1)T} e^{\mtx{A}((k + 1)T - \tau)}\mtx{B}\mtx{u}(\tau)
    \,d\tau \\
  \mtx{x}_{k+1} &= e^{\mtx{A}(k + 1)T}\mtx{x}(0) +
    \int_0^{kT} e^{\mtx{A}((k + 1)T - \tau)}\mtx{B}\mtx{u}(\tau) \,d\tau +
    \int_{kT}^{(k + 1)T} e^{\mtx{A}(kT + T - \tau)}\mtx{B}\mtx{u}(\tau) \,d\tau
    \\
  \mtx{x}_{k+1} &= e^{\mtx{A}T} \underbrace{\left(e^{\mtx{A}kT}\mtx{x}(0) +
    \int_0^{kT} e^{\mtx{A}(kT - \tau)}\mtx{B}\mtx{u}(\tau)
    \,d\tau\right)}_{\mtx{x}_k} +
    \int_{kT}^{(k + 1)T} e^{\mtx{A}(kT + T - \tau)}\mtx{B}\mtx{u}(\tau) \,d\tau
\end{align*}

We assume that $\mtx{u}$ is constant during each timestep, so it can be pulled
out of the integral.

\begin{equation*}
  \mtx{x}_{k+1} = e^{\mtx{A}T}\mtx{x}_k +
    \left(\int_{kT}^{(k + 1)T} e^{\mtx{A}(kT + T - \tau)} \,d\tau\right)
    \mtx{B}\mtx{u}_k
\end{equation*}

The second term can be simplified by substituting it with the function
$v(\tau) = kT + T - \tau$. Note that $d\tau = -dv$.

\begin{align*}
  \mtx{x}_{k+1} &= e^{\mtx{A}T}\mtx{x}_k -
    \left(\int_{v(kT)}^{v((k + 1)T)} e^{\mtx{A}v} \,dv\right)
    \mtx{B}\mtx{u}_k \\
  \mtx{x}_{k+1} &= e^{\mtx{A}T}\mtx{x}_k -
    \left(\int_T^0 e^{\mtx{A}v} \,dv\right) \mtx{B}\mtx{u}_k \\
  \mtx{x}_{k+1} &= e^{\mtx{A}T}\mtx{x}_k +
    \left(\int_0^T e^{\mtx{A}v} \,dv\right) \mtx{B}\mtx{u}_k \\
  \mtx{x}_{k+1} &= e^{\mtx{A}T}\mtx{x}_k +
    \mtx{A}^{-1}e^{\mtx{A}v} \rvert_0^T \mtx{B}\mtx{u}_k \\
  \mtx{x}_{k+1} &= e^{\mtx{A}T}\mtx{x}_k +
    \mtx{A}^{-1}(e^{\mtx{A}T} - e^{\mtx{A}0}) \mtx{B}\mtx{u}_k \\
  \mtx{x}_{k+1} &= e^{\mtx{A}T}\mtx{x}_k +
    \mtx{A}^{-1}(e^{\mtx{A}T} - \mtx{I}) \mtx{B}\mtx{u}_k
\end{align*}

which is an exact solution to the \gls{discretization} problem.

\section{Kalman filter as Luenberger observer}
\label{sec:deriv_kalman_luenberger}

A Luenberger \gls{observer} is defined as

\begin{align}
  \hat{\mtx{x}}_{k+1}^+ &= \mtx{A}\hat{\mtx{x}}_k^- + \mtx{B}\mtx{u}_k + \mtx{L}
    (\mtx{y}_k - \hat{\mtx{y}}_k) \label{eq:luenberger1} \\
  \hat{\mtx{y}}_k &= \mtx{C} \hat{\mtx{x}}_k^- \label{eq:luenberger2}
\end{align}

where a superscript of minus denotes \textit{a priori} and plus denotes
\textit{a posteriori} estimate. Combining equation \eqref{eq:luenberger1} and
equation \eqref{eq:luenberger2} gives

\begin{equation} \label{eq:luenberger}
  \hat{\mtx{x}}_{k+1}^+ = \mtx{A}\hat{\mtx{x}}_k^- + \mtx{B}\mtx{u}_k + \mtx{L}
    (\mtx{y}_k - \mtx{C}\hat{\mtx{x}}_k^-)
\end{equation}

The following is a Kalman filter that considers the current update step and the
next predict step together rather than the current predict step and current
update step.

\begin{align}
  \text{Update step} \nonumber \\
  \mtx{K}_k &= \mtx{P}_k^- \mtx{C}^T (\mtx{C}\mtx{P}_k^- \mtx{C}^T +
    \mtx{R})^{-1} \\
  \hat{\mtx{x}}_k^+ &= \hat{\mtx{x}}_k^- + \mtx{K}_k(\mtx{y}_k -
    \mtx{C}\hat{\mtx{x}}_k^-) \label{eq:post2_x} \\
  \mtx{P}_k^+ &= (\mtx{I} - \mtx{K}_k\mtx{C})\mtx{P}_k^- \\
  \text{Predict step} \nonumber \\
  \hat{\mtx{x}}_{k+1}^+ &= \mtx{A}\hat{\mtx{x}}_k^+ + \mtx{B}\mtx{u}_k
    \label{eq:pre2_x} \\
  \mtx{P}_{k+1}^- &= \mtx{A} \mtx{P}_k^+ \mtx{A}^T +
    \mtx{\Gamma}\mtx{Q}\mtx{\Gamma}^T
\end{align}

Substitute equation \eqref{eq:post2_x} into equation \eqref{eq:pre2_x}.

\begin{align*}
  \hat{\mtx{x}}_{k+1}^+ &= \mtx{A}(\hat{\mtx{x}}_k^- + \mtx{K}_k(\mtx{y}_k -
    \mtx{C}\hat{\mtx{x}}_k^-)) + \mtx{B}\mtx{u}_k \\
  \hat{\mtx{x}}_{k+1}^+ &= \mtx{A}\mtx{x}_k^- + \mtx{A}\mtx{K}_k(\mtx{y}_k -
    \mtx{C}\hat{\mtx{x}}_k^-) + \mtx{B}\mtx{u}_k \\
  \hat{\mtx{x}}_{k+1}^+ &= \mtx{A}\hat{\mtx{x}}_k^- + \mtx{B}\mtx{u}_k +
    \mtx{A}\mtx{K}_k(\mtx{y}_k - \mtx{C}\hat{\mtx{x}}_k^-)
\end{align*}

Let $\mtx{L} = \mtx{A} \mtx{K}_k$.

\begin{equation}
  \hat{\mtx{x}}_{k+1}^+ = \mtx{A}\hat{\mtx{x}}_k^- + \mtx{B}\mtx{u}_k + \mtx{L}
    (\mtx{y}_k - \mtx{C}\hat{\mtx{x}}_k^-)
\end{equation}

which matches equation \eqref{eq:luenberger}. Therefore, the eigenvalues of the
Kalman filter \gls{observer} can be obtained by

\begin{align}
  &\eig(\mtx{A} - \mtx{L}\mtx{C}) \nonumber \\
  &\eig(\mtx{A} - (\mtx{A}\mtx{K}_k)(\mtx{C})) \nonumber \\
  &\eig(\mtx{A}(\mtx{I} - \mtx{K}_k\mtx{C}))
\end{align}

\subsection{Luenberger observer with separate prediction and update}
\label{subsec:deriv_luenberger_separate}

To run a Luenberger \gls{observer} with separate prediction and update steps,
substitute the relationship between the Luenberger \gls{observer} and Kalman
filter matrices derived above into the Kalman filter equations.

Appendix \ref{sec:deriv_kalman_luenberger} shows that
$\mtx{L} = \mtx{A}\mtx{K}_k$. Since $\mtx{L}$ and $\mtx{A}$ are constant, one
must assume $\mtx{K}_k$ has reached steady-state. Then,
$\mtx{K} = \mtx{A}^{-1}\mtx{L}$. Substitute this into the Kalman filter update
equation.

\begin{align*}
  \hat{\mtx{x}}_{k+1}^+ &= \hat{\mtx{x}}_{k+1}^- + \mtx{K}(\mtx{y}_{k+1} -
    \mtx{C}\hat{\mtx{x}}_{k+1}^-) \\
  \hat{\mtx{x}}_{k+1}^+ &= \hat{\mtx{x}}_{k+1}^- + \mtx{A}^{-1}\mtx{L}
    (\mtx{y}_{k+1} - \mtx{C}\hat{\mtx{x}}_{k+1}^-)
\end{align*}

Substitute in equation \eqref{eq:z_obsv_y}.

\begin{equation*}
  \hat{\mtx{x}}_{k+1}^+ = \hat{\mtx{x}}_{k+1}^- + \mtx{A}^{-1}\mtx{L}
    (\mtx{y}_{k+1} - \hat{\mtx{y}}_{k+1})
\end{equation*}

The predict step is the same as the Kalman filter's. Therefore, a Luenberger
\gls{observer} run with prediction and update steps is written as follows.

\begin{align}
  \text{Predict step} \nonumber \\
  \hat{\mtx{x}}_{k+1}^- &= \mtx{A}\hat{\mtx{x}}_k^- + \mtx{B}\mtx{u}_k \\
  \text{Update step} \nonumber \\
  \hat{\mtx{x}}_{k+1}^+ &= \hat{\mtx{x}}_{k+1}^- + \mtx{A}^{-1}\mtx{L}
    (\mtx{y}_{k+1} - \hat{\mtx{y}}_{k+1}) \\
  \hat{\mtx{y}}_{k+1} &= \mtx{C} \hat{\mtx{x}}_{k+1}^-
\end{align}

\section{Trapezoidal motion profile}
\label{sec:deriv_trapezoid_profile}
