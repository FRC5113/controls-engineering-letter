\chapterimage{appendices.jpg}{Sunset in an airplane over New Mexico}

\chapter{QR-weighted linear plant inversion}

\section{Necessary theorems}

The following theorem and corollary will be needed to derive the QR-weighted
linear plant inversion equation.
\begin{theorem}
  $\frac{\partial (\mat{A}\mat{x} + \mat{b})\T\mat{C}
    (\mat{D}\mat{x} + \mat{e})}{\partial\mat{x}} =
    \mat{A}\T\mat{C}(\mat{D}\mat{x} + \mat{e}) + \mat{D}\T\mat{C}\T
    (\mat{A}\mat{x} + \mat{b})$
\end{theorem}
\begin{corollary}
  \label{cor:partial_ax_b}

  $\frac{\partial (\mat{A}\mat{x} + \mat{b})\T\mat{C}
    (\mat{A}\mat{x} + \mat{b})}{\partial\mat{x}} =
    2\mat{A}\T\mat{C}(\mat{A}\mat{x} + \mat{b})$ where $\mat{C}$ is symmetric.

  Proof:
  \begin{align*}
    \frac{\partial (\mat{A}\mat{x} + \mat{b})\T\mat{C}
      (\mat{A}\mat{x} + \mat{b})}{\partial\mat{x}} &=
      \mat{A}\T\mat{C}(\mat{A}\mat{x} + \mat{b}) + \mat{A}\T\mat{C}\T
      (\mat{A}\mat{x} + \mat{b}) \\
    \frac{\partial (\mat{A}\mat{x} + \mat{b})\T\mat{C}
      (\mat{A}\mat{x} + \mat{b})}{\partial\mat{x}} &=
      (\mat{A}\T\mat{C} + \mat{A}\T\mat{C}\T)(\mat{A}\mat{x} + \mat{b})
  \end{align*}

  $\mat{C}$ is symmetric, so
  \begin{align*}
    \frac{\partial (\mat{A}\mat{x} + \mat{b})\T\mat{C}
      (\mat{A}\mat{x} + \mat{b})}{\partial\mat{x}} &=
      (\mat{A}\T\mat{C} + \mat{A}\T\mat{C})(\mat{A}\mat{x} + \mat{b}) \\
    \frac{\partial (\mat{A}\mat{x} + \mat{b})\T\mat{C}
      (\mat{A}\mat{x} + \mat{b})}{\partial\mat{x}} &=
      2\mat{A}\T\mat{C}(\mat{A}\mat{x} + \mat{b})
  \end{align*}
\end{corollary}

\section{Setup}

Let's start with the equation for the \gls{reference} dynamics
\begin{equation*}
  \mat{r}_{k+1} = \mat{A}\mat{r}_k + \mat{B}\mat{u}_k
\end{equation*}

where $\mat{u}_k$ is the feedforward input. Note that this feedforward equation
does not and should not take into account any feedback terms. We want to find
the optimal $\mat{u}_k$ such that we minimize the \gls{tracking} error between
$\mat{r}_{k+1}$ and $\mat{r}_k$.
\begin{equation*}
  \mat{r}_{k+1} - \mat{A}\mat{r}_k = \mat{B}\mat{u}_k
\end{equation*}

To solve for $\mat{u}_k$, we need to take the inverse of the nonsquare matrix
$\mat{B}$. This isn't possible, but we can find the pseudoinverse given some
constraints on the \gls{state} \gls{tracking} error and \gls{control effort}. To
find the optimal solution for these sorts of trade-offs, one can define a cost
function and attempt to minimize it. To do this, we'll first solve the
expression for $\mat{0}$.
\begin{equation*}
  \mat{0} = \mat{B}\mat{u}_k - (\mat{r}_{k+1} - \mat{A}\mat{r}_k)
\end{equation*}

This expression will be the \gls{state} \gls{tracking} cost we use in our cost
function.

Our cost function will use an $H_2$ norm with $\mat{Q}$ as the \gls{state} cost
matrix with dimensionality $states \times states$ and $\mat{R}$ as the
\gls{control input} cost matrix with dimensionality $inputs \times inputs$.
\begin{equation*}
  \mat{J} = (\mat{B}\mat{u}_k - (\mat{r}_{k+1} - \mat{A}\mat{r}_k))\T \mat{Q}
    (\mat{B}\mat{u}_k - (\mat{r}_{k+1} - \mat{A}\mat{r}_k)) +
    \mat{u}_k\T\mat{R}\mat{u}_k
\end{equation*}

\section{Minimization}

Given theorem \ref{thm:partial_xax} and corollary \ref{cor:partial_ax_b}, find
the minimum of $\mat{J}$ by taking the partial derivative with respect to
$\mat{u}_k$ and setting the result to $\mat{0}$.
\begin{align*}
  \frac{\partial\mat{J}}{\partial\mat{u}_k} &= 2\mat{B}\T\mat{Q}
    (\mat{B}\mat{u}_k - (\mat{r}_{k+1} - \mat{A}\mat{r}_k)) +
    2\mat{R}\mat{u}_k \\
  \mat{0} &= 2\mat{B}\T\mat{Q}
    (\mat{B}\mat{u}_k - (\mat{r}_{k+1} - \mat{A}\mat{r}_k)) +
    2\mat{R}\mat{u}_k \\
  \mat{0} &= \mat{B}\T\mat{Q}
    (\mat{B}\mat{u}_k - (\mat{r}_{k+1} - \mat{A}\mat{r}_k)) +
    \mat{R}\mat{u}_k \\
  \mat{0} &= \mat{B}\T\mat{Q}\mat{B}\mat{u}_k -
    \mat{B}\T\mat{Q}(\mat{r}_{k+1} - \mat{A}\mat{r}_k) + \mat{R}\mat{u}_k \\
  \mat{B}\T\mat{Q}\mat{B}\mat{u}_k + \mat{R}\mat{u}_k &=
    \mat{B}\T\mat{Q}(\mat{r}_{k+1} - \mat{A}\mat{r}_k) \\
  (\mat{B}\T\mat{Q}\mat{B} + \mat{R})\mat{u}_k &=
    \mat{B}\T\mat{Q}(\mat{r}_{k+1} - \mat{A}\mat{r}_k) \\
  \mat{u}_k &= (\mat{B}\T\mat{Q}\mat{B} + \mat{R})^{-1}
    \mat{B}\T\mat{Q}(\mat{r}_{k+1} - \mat{A}\mat{r}_k)
\end{align*}
\begin{theorem}[QR-weighted linear plant inversion]
  Given the discrete model
  $\mat{x}_{k+1} = \mat{A}\mat{x}_k + \mat{B}\mat{u}_k$, the plant inversion
  feedforward is
  \begin{equation*}
    \mat{u}_k = \mat{K}_{ff} (\mat{r}_{k+1} - \mat{A}\mat{r}_k)
  \end{equation*}

  where
  $\mat{K}_{ff} = (\mat{B}\T\mat{Q}\mat{B} + \mat{R})^{-1}\mat{B}\T\mat{Q}$,
  $\mat{r}_{k+1}$ is the reference at the next timestep, and $\mat{r}_k$ is the
  reference at the current timestep.
\end{theorem}

Figure \ref{fig:case_study_qr_ff} shows \gls{plant} inversion applied to a
second-order CIM motor model.
\begin{svg}{build/\partpath/case_study_qr_ff}
  \caption{Second-order CIM motor response with plant inversion}
  \label{fig:case_study_qr_ff}
\end{svg}

\Gls{plant} inversion isn't as effective with both $\mat{Q}$ and $\mat{R}$ cost
because the $\mat{R}$ matrix penalized \gls{control effort}. The \gls{reference}
\gls{tracking} with no cost matrices is much better.
