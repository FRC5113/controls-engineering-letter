\section{Kalman filter}

So far, we've derived equations for updating the expected value and state
covariance without measurements and how to incorporate measurements into an
initial \gls{state} optimally. Now, we'll combine these concepts to produce an
estimator which minimizes the error covariance for linear \glspl{system}.

\subsection{Derivations}
\index{state-space observers!Kalman filter!derivations}

Given the \textit{a posteriori} update equation
$\hat{\mtx{x}}_{k+1}^+ = \hat{\mtx{x}}_{k+1}^- + \mtx{K}_{k+1}(\mtx{y}_{k+1} -
\hat{\mtx{y}}_{k+1})$, we want to find the value of $\mtx{K}_{k+1}$ that
minimizes the \textit{a posteriori} estimate covariance (the error covariance)
because this minimizes the estimation error.

\subsubsection{\textit{a posteriori} estimate covariance update equation}

The following is the definition of the \textit{a posteriori} estimate covariance
matrix.
\begin{equation*}
  \mtx{P}_{k+1}^+ = \cov(\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^+)
\end{equation*}

Substitute in the \textit{a posteriori} update equation and expand the
measurement equations.
\begin{align*}
  \mtx{P}_{k+1}^+ = \cov(&\mtx{x}_{k+1} - (\hat{\mtx{x}}_{k+1}^- +
    \mtx{K}_{k+1}(\mtx{y}_{k+1} - \hat{\mtx{y}}_{k+1}))) \\
  \mtx{P}_{k+1}^+ = \cov(&\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^- -
    \mtx{K}_{k+1}(\mtx{y}_{k+1} - \hat{\mtx{y}}_{k+1})) \\
  \mtx{P}_{k+1}^+ = \cov(&\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^- - \\
    &\mtx{K}_{k+1}((\mtx{C}_{k+1} \mtx{x}_{k+1} + \mtx{D}_{k+1} \mtx{u}_{k+1} +
      \mtx{v}_{k+1}) - (\mtx{C}_{k+1} \hat{\mtx{x}}_{k+1}^- +
      \mtx{D}_{k+1}\mtx{u}_{k+1}))) \\
  \mtx{P}_{k+1}^+ = \cov(&\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^- - \\
    &\mtx{K}_{k+1}(\mtx{C}_{k+1} \mtx{x}_{k+1} + \mtx{D}_{k+1} \mtx{u}_{k+1} +
      \mtx{v}_{k+1} - \mtx{C}_{k+1} \hat{\mtx{x}}_{k+1}^- -
      \mtx{D}_{k+1}\mtx{u}_{k+1}))
\end{align*}

Reorder terms.
\begin{align*}
  \mtx{P}_{k+1}^+ = \cov(&\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^- - \\
    &\mtx{K}_{k+1}(\mtx{C}_{k+1} \mtx{x}_{k+1} -
      \mtx{C}_{k+1} \hat{\mtx{x}}_{k+1}^- + \mtx{D}_{k+1} \mtx{u}_{k+1} -
      \mtx{D}_{k+1}\mtx{u}_{k+1} + \mtx{v}_{k+1}))
\end{align*}

The $\mtx{D}_{k+1}\mtx{u}_{k+1}$ terms cancel.
\begin{equation*}
  \mtx{P}_{k+1}^+ = \cov(\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^- -
    \mtx{K}_{k+1}(\mtx{C}_{k+1}\mtx{x}_{k+1} -
    \mtx{C}_{k+1} \hat{\mtx{x}}_{k+1}^- + \mtx{v}_{k+1}))
\end{equation*}

Distribute $\mtx{K}_{k+1}$ to $\mtx{v}_{k+1}$.
\begin{equation*}
  \mtx{P}_{k+1}^+ = \cov(\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^- -
    \mtx{K}_{k+1}(\mtx{C}_{k+1}\mtx{x}_{k+1} -
    \mtx{C}_{k+1} \hat{\mtx{x}}_{k+1}^-) - \mtx{K}_{k+1}\mtx{v}_{k+1})
\end{equation*}

Factor out $\mtx{C}_{k+1}$.
\begin{equation*}
  \mtx{P}_{k+1}^+ = \cov(\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^- -
    \mtx{K}_{k+1}\mtx{C}_{k+1}(\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^-) -
    \mtx{K}_{k+1}\mtx{v}_{k+1})
\end{equation*}

Factor out $\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^-$ to the right.
\begin{equation*}
  \mtx{P}_{k+1}^+ = \cov((\mtx{I} - \mtx{K}_{k+1}\mtx{C}_{k+1})
    (\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^-) - \mtx{K}_{k+1}\mtx{v}_{k+1})
\end{equation*}

Covariance is a linear operator, so it can be applied to each term separately.
Covariance squares terms internally, so the negative sign on
$\mtx{K}_{k+1}\mtx{v}_{k+1}$ is removed.
\begin{equation*}
  \mtx{P}_{k+1}^+ = \cov((\mtx{I} - \mtx{K}_{k+1}\mtx{C}_{k+1})
    (\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^-)) + \cov(\mtx{K}_{k+1}\mtx{v}_{k+1})
\end{equation*}

Now just evaluate the covariances.
\begin{align*}
  \mtx{P}_{k+1}^+ &= (\mtx{I} - \mtx{K}_{k+1}\mtx{C}_{k+1})
    \cov(\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^-)
    (\mtx{I} - \mtx{K}_{k+1}\mtx{C}_{k+1})^T + \mtx{K}_{k+1}cov(\mtx{v}_{k+1})
    \mtx{K}_{k+1}^T \\
  \mtx{P}_{k+1}^+ &= (\mtx{I} - \mtx{K}_{k+1}\mtx{C}_{k+1})\mtx{P}_{k+1}^-
    (\mtx{I} - \mtx{K}_{k+1}\mtx{C}_{k+1})^T + \mtx{K}_{k+1}\mtx{R}_{k+1}
    \mtx{K}_{k+1}^T
\end{align*}

\subsubsection{Finding the optimal Kalman gain}

The error in the \textit{a posteriori} \gls{state} estimation is
$\mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1}^-$. We want to minimize the expected value
of the square of the magnitude of this vector. This is equivalent to minimizing
the trace of the a posteriori estimate covariance matrix $\mtx{P}_{k+1}^+$.

We'll start with the equation for $\mtx{P}_{k+1}^+$.
\begin{equation*}
  \mtx{P}_{k+1}^+ = (\mtx{I} - \mtx{K}_{k+1}\mtx{C}_{k+1})\mtx{P}_{k+1}^-
    (\mtx{I} - \mtx{K}_{k+1}\mtx{C}_{k+1})^T + \mtx{K}_{k+1}\mtx{R}_{k+1}
    \mtx{K}_{k+1}^T
\end{equation*}

We're going to expand the equation for $\mtx{P}_{k+1}^+$ and collect terms.
First, multiply in $\mtx{P}_{k+1}^-$.
\begin{equation*}
  \mtx{P}_{k+1}^+ =
    (\mtx{P}_{k+1}^- - \mtx{K}_{k+1}\mtx{C}_{k+1}\mtx{P}_{k+1}^-)
    (\mtx{I} - \mtx{K}_{k+1}\mtx{C}_{k+1})^T + \mtx{K}_{k+1}\mtx{R}_{k+1}
    \mtx{K}_{k+1}^T
\end{equation*}

Tranpose each term in $\mtx{I} - \mtx{K}_{k+1}\mtx{C}_{k+1}$. $\mtx{I}$ is
symmetric, so its transpose is dropped.
\begin{equation*}
  \mtx{P}_{k+1}^+ =
    (\mtx{P}_{k+1}^- - \mtx{K}_{k+1}\mtx{C}_{k+1}\mtx{P}_{k+1}^-)
    (\mtx{I} - \mtx{C}_{k+1}^T\mtx{K}_{k+1}^T) +
    \mtx{K}_{k+1}\mtx{R}_{k+1} \mtx{K}_{k+1}^T
\end{equation*}

Multiply in $\mtx{I} - \mtx{C}_{k+1}^T\mtx{K}_{k+1}^T$.
\begin{equation*}
  \mtx{P}_{k+1}^+ =
    \mtx{P}_{k+1}^-(\mtx{I} - \mtx{C}_{k+1}^T\mtx{K}_{k+1}^T) -
    \mtx{K}_{k+1}\mtx{C}_{k+1}\mtx{P}_{k+1}^-
    (\mtx{I} - \mtx{C}_{k+1}^T\mtx{K}_{k+1}^T) +
    \mtx{K}_{k+1}\mtx{R}_{k+1} \mtx{K}_{k+1}^T
\end{equation*}

Expand terms.
\begin{align*}
  \mtx{P}_{k+1}^+ =~&
    \mtx{P}_{k+1}^- - \mtx{P}_{k+1}^-\mtx{C}_{k+1}^T\mtx{K}_{k+1}^T -
    \mtx{K}_{k+1}\mtx{C}_{k+1}\mtx{P}_{k+1}^- +
    \mtx{K}_{k+1}\mtx{C}_{k+1}\mtx{P}_{k+1}^-\mtx{C}_{k+1}^T\mtx{K}_{k+1}^T + \\
    &\mtx{K}_{k+1}\mtx{R}_{k+1} \mtx{K}_{k+1}^T
\end{align*}

Factor out $\mtx{K}_{k+1}$ and $\mtx{K}_{k+1}^T$.
\begin{align}
  \mtx{P}_{k+1}^+ =~&
    \mtx{P}_{k+1}^- - \mtx{P}_{k+1}^-\mtx{C}_{k+1}^T\mtx{K}_{k+1}^T -
    \mtx{K}_{k+1}\mtx{C}_{k+1}\mtx{P}_{k+1}^- + \nonumber \\
    &\mtx{K}_{k+1}(\mtx{C}_{k+1}\mtx{P}_{k+1}^-\mtx{C}_{k+1}^T +
    \mtx{R}_{k+1})\mtx{K}_{k+1}^T \nonumber \\
  \mtx{P}_{k+1}^+ =~&
    \mtx{P}_{k+1}^- - \mtx{P}_{k+1}^-\mtx{C}_{k+1}^T\mtx{K}_{k+1}^T -
    \mtx{K}_{k+1}\mtx{C}_{k+1}\mtx{P}_{k+1}^- +
    \mtx{K}_{k+1}\mtx{S}_{k+1}\mtx{K}_{k+1}^T \label{eq:post_p_update}
\end{align}

Now take the trace.
\begin{equation*}
  \tr(\mtx{P}_{k+1}^+) =
    \tr(\mtx{P}_{k+1}^-) - \tr(\mtx{P}_{k+1}^-\mtx{C}_{k+1}^T\mtx{K}_{k+1}^T) -
    \tr(\mtx{K}_{k+1}\mtx{C}_{k+1}\mtx{P}_{k+1}^-) +
    \tr(\mtx{K}_{k+1}\mtx{S}_{k+1}\mtx{K}_{k+1}^T)
\end{equation*}

Transpose one of the terms twice.
\begin{equation*}
  \tr(\mtx{P}_{k+1}^+) = \tr(\mtx{P}_{k+1}^-) -
    \tr((\mtx{K}_{k+1}\mtx{C}_{k+1}\mtx{P}_{k+1}^{-T})^T) -
    \tr(\mtx{K}_{k+1}\mtx{C}_{k+1}\mtx{P}_{k+1}^-) +
    \tr(\mtx{K}_{k+1}\mtx{S}_{k+1}\mtx{K}_{k+1}^T)
\end{equation*}

$\mtx{P}_{k+1}^-$ is symmetric, so we can drop the transpose.
\begin{equation*}
  \tr(\mtx{P}_{k+1}^+) = \tr(\mtx{P}_{k+1}^-) -
    \tr((\mtx{K}_{k+1}\mtx{C}_{k+1}\mtx{P}_{k+1}^-)^T) -
    \tr(\mtx{K}_{k+1}\mtx{C}_{k+1}\mtx{P}_{k+1}^-) +
    \tr(\mtx{K}_{k+1}\mtx{S}_{k+1}\mtx{K}_{k+1}^T)
\end{equation*}

The trace of a matrix is equal to the trace of its transpose since the elements
used in the trace are on the diagonal.
\begin{align*}
  \tr(\mtx{P}_{k+1}^+) &= \tr(\mtx{P}_{k+1}^-) -
    \tr(\mtx{K}_{k+1}\mtx{C}_{k+1}\mtx{P}_{k+1}^-) -
    \tr(\mtx{K}_{k+1}\mtx{C}_{k+1}\mtx{P}_{k+1}^-) +
    \tr(\mtx{K}_{k+1}\mtx{S}_{k+1}\mtx{K}_{k+1}^T) \\
  \tr(\mtx{P}_{k+1}^+) &= \tr(\mtx{P}_{k+1}^-) -
    2\tr(\mtx{K}_{k+1}\mtx{C}_{k+1}\mtx{P}_{k+1}^-) +
    \tr(\mtx{K}_{k+1}\mtx{S}_{k+1}\mtx{K}_{k+1}^T)
\end{align*}

Given theorems \ref{thm:partial_tr_aba} and \ref{thm:partial_tr_ac}
\begin{theorem}
  \label{thm:partial_tr_aba}

  $\frac{\partial}{\partial\mtx{A}}\tr(\mtx{A}\mtx{B}\mtx{A}^T) =
    2\mtx{A}\mtx{B}$ where $\mtx{B}$ is symmetric.
\end{theorem}
\begin{theorem}
  \label{thm:partial_tr_ac}

  $\frac{\partial}{\partial\mtx{A}}\tr(\mtx{A}\mtx{C}) = \mtx{C}^T$
\end{theorem}

find the minimum of the trace of $\mtx{P}_{k+1}^+$ by taking the partial
derivative with respect to $\mtx{K}$ and setting the result to $\mtx{0}$.
\begin{align*}
  \frac{\partial\tr(\mtx{P}_{k+1}^+)}{\partial\mtx{K}} &=
    \mtx{0} - 2(\mtx{C}_{k+1}\mtx{P}_{k+1}^-)^T + 2\mtx{K}_{k+1}\mtx{S}_{k+1} \\
  \frac{\partial\tr(\mtx{P}_{k+1}^+)}{\partial\mtx{K}} &=
    -2\mtx{P}_{k+1}^{-T}\mtx{C}_{k+1}^T + 2\mtx{K}_{k+1}\mtx{S}_{k+1} \\
  \frac{\partial\tr(\mtx{P}_{k+1}^+)}{\partial\mtx{K}} &=
    -2\mtx{P}_{k+1}^-\mtx{C}_{k+1}^T + 2\mtx{K}_{k+1}\mtx{S}_{k+1} \\
  \mtx{0} &= -2\mtx{P}_{k+1}^-\mtx{C}_{k+1}^T + 2\mtx{K}_{k+1}\mtx{S}_{k+1} \\
  2\mtx{K}_{k+1}\mtx{S}_{k+1} &= 2\mtx{P}_{k+1}^-\mtx{C}_{k+1}^T \\
  \mtx{K}_{k+1}\mtx{S}_{k+1} &= \mtx{P}_{k+1}^-\mtx{C}_{k+1}^T \\
  \mtx{K}_{k+1} &= \mtx{P}_{k+1}^-\mtx{C}_{k+1}^T\mtx{S}_{k+1}^{-1}
\end{align*}

This is the optimal Kalman gain.

\subsubsection{Simplified \textit{a priori} estimate covariance update equation}

If the optimal Kalman gain is used, the \textit{a posteriori} estimate
covariance matrix update equation can be simplified. First, we'll manipulate the
equation for the optimal Kalman gain.
\begin{align*}
  \mtx{K}_{k+1} &= \mtx{P}_{k+1}^-\mtx{C}_{k+1}^T\mtx{S}_{k+1}^{-1} \\
  \mtx{K}_{k+1}\mtx{S}_{k+1} &= \mtx{P}_{k+1}^-\mtx{C}_{k+1}^T \\
  \mtx{K}_{k+1}\mtx{S}_{k+1}\mtx{K}_{k+1}^T &=
    \mtx{P}_{k+1}^-\mtx{C}_{k+1}^T\mtx{K}_{k+1}^T
\end{align*}

Now we'll substitute it into equation \eqref{eq:post_p_update}.
\begin{align*}
  \mtx{P}_{k+1}^+ &=
    \mtx{P}_{k+1}^- - \mtx{P}_{k+1}^-\mtx{C}_{k+1}^T\mtx{K}_{k+1}^T -
    \mtx{K}_{k+1}\mtx{C}_{k+1}\mtx{P}_{k+1}^- +
    \mtx{K}_{k+1}\mtx{S}_{k+1}\mtx{K}_{k+1}^T \\
  \mtx{P}_{k+1}^+ &=
    \mtx{P}_{k+1}^- - \mtx{P}_{k+1}^-\mtx{C}_{k+1}^T\mtx{K}_{k+1}^T -
    \mtx{K}_{k+1}\mtx{C}_{k+1}\mtx{P}_{k+1}^- +
    (\mtx{P}_{k+1}^-\mtx{C}_{k+1}^T\mtx{K}_{k+1}^T) \\
  \mtx{P}_{k+1}^+ &=
    \mtx{P}_{k+1}^- - \mtx{K}_{k+1}\mtx{C}_{k+1}\mtx{P}_{k+1}^-
\end{align*}

Factor out $\mtx{P}_{k+1}^-$ to the right.
\begin{equation*}
  \mtx{P}_{k+1}^+ = (\mtx{I} - \mtx{K}_{k+1}\mtx{C}_{k+1})\mtx{P}_{k+1}^-
\end{equation*}

\subsection{Predict and update equations}

Now that we've derived all the pieces we need, we can finally write all the
equations for a Kalman filter. Theorem \ref{thm:kalman_filter} shows the predict
and update steps for a Kalman filter at the $k^{th}$ timestep.

\index{state-space observers!Kalman filter!equations}
\begin{theorem}[Kalman filter]
  \label{thm:kalman_filter}

  \begin{align}
    \text{Predict step} \nonumber \\
    \hat{\mtx{x}}_{k+1}^- &= \mtx{A}\hat{\mtx{x}}_k + \mtx{B} \mtx{u}_k \\
    \mtx{P}_{k+1}^- &= \mtx{A} \mtx{P}_k^- \mtx{A}^T +
      \mtx{\Gamma}\mtx{Q}\mtx{\Gamma}^T \\
    \text{Update step} \nonumber \\
    \mtx{K}_{k+1} &=
      \mtx{P}_{k+1}^- \mtx{C}^T (\mtx{C}\mtx{P}_{k+1}^- \mtx{C}^T +
      \mtx{R})^{-1} \\
    \hat{\mtx{x}}_{k+1}^+ &=
      \hat{\mtx{x}}_{k+1}^- + \mtx{K}_{k+1}(\mtx{y}_{k+1} -
      \mtx{C} \hat{\mtx{x}}_{k+1}^- - \mtx{D}\mtx{u}_{k+1}) \\
    \mtx{P}_{k+1}^+ &= (\mtx{I} - \mtx{K}_{k+1}\mtx{C})\mtx{P}_{k+1}^-
  \end{align}

  \begin{figurekey}
    \begin{tabular}{llll}
      $\mtx{A}$ & system matrix & $\hat{\mtx{x}}$ & state estimate vector \\
      $\mtx{B}$ & input matrix       & $\mtx{u}$ & input vector \\
      $\mtx{C}$ & output matrix      & $\mtx{y}$ & output vector \\
      $\mtx{D}$ & feedthrough matrix & $\mtx{\Gamma}$ & process noise intensity
        vector \\
      $\mtx{P}$ & error covariance matrix & $\mtx{Q}$ & process noise covariance
        matrix \\
      $\mtx{K}$ & Kalman gain matrix & $\mtx{R}$ & measurement noise covariance
        matrix
    \end{tabular}
  \end{figurekey}

  where a superscript of minus denotes \textit{a priori} and plus denotes
  \textit{a posteriori} estimate (before and after update respectively).
\end{theorem}

$\mtx{C}$, $\mtx{D}$, $\mtx{Q}$, and $\mtx{R}$ from the equations derived
earlier are made constants here.
\begin{remark}
  To implement a discrete time Kalman filter from a continuous model, the model
  and continuous time $\mtx{Q}$ and $\mtx{R}$ matrices can be
  \glslink{discretization}{discretized} using theorem \ref{thm:zoh_ss}.
\end{remark}
\begin{booktable}
  \begin{tabular}{|ll|ll|}
    \hline
    \rowcolor{headingbg}
    \textbf{Matrix} & \textbf{Rows $\times$ Columns} &
    \textbf{Matrix} & \textbf{Rows $\times$ Columns} \\
    \hline
    $\mtx{A}$ & states $\times$ states & $\hat{\mtx{x}}$ & states $\times$ 1 \\
    $\mtx{B}$ & states $\times$ inputs & $\mtx{u}$ & inputs $\times$ 1 \\
    $\mtx{C}$ & outputs $\times$ states & $\mtx{y}$ & outputs $\times$ 1 \\
    $\mtx{D}$ & outputs $\times$ inputs & $\mtx{\Gamma}$ & states $\times$ 1 \\
    $\mtx{P}$ & states $\times$ states & $\mtx{Q}$ & states $\times$ states \\
    $\mtx{K}$ & states $\times$ outputs & $\mtx{R}$ & outputs $\times$ outputs
      \\
    \hline
  \end{tabular}
  \caption{Kalman filter matrix dimensions}
\end{booktable}

Unknown \glspl{state} in a Kalman filter are generally represented by a Wiener
(pronounced VEE-ner) process\footnote{Explaining why we use the Wiener process
would require going much more in depth into stochastic processes and It\^{o}
calculus, which is outside the scope of this book.}. This process has the
property that its variance increases linearly with time $t$.

\subsection{Setup}
\index{state-space observers!Kalman filter!setup}

\subsubsection{Equations to model}

The following example \gls{system} will be used to describe how to define and
initialize the matrices for a Kalman filter.

A robot is between two parallel walls. It starts driving from one wall to the
other at a velocity of $0.8 cm/s$ and uses ultrasonic sensors to provide noisy
measurements of the distances to the walls in front of and behind it. To
estimate the distance between the walls, we will define three \glspl{state}:
robot position, robot velocity, and distance between the walls.
\begin{align}
  x_{k+1} &= x_k + v_k \Delta T \\
  v_{k+1} &= v_k \\
  x_{k+1}^w &= x_k^w
\end{align}

This can be converted to the following state-space \gls{model}.
\begin{equation}
  \mtx{x}_k =
  \begin{bmatrix}
    x_k \\
    v_k \\
    x_k^w
  \end{bmatrix}
\end{equation}
\begin{equation}
  \mtx{x}_{k+1} =
  \begin{bmatrix}
    1 & 1 & 0 \\
    0 & 0 & 0 \\
    0 & 0 & 1
  \end{bmatrix} \mtx{x}_k +
  \begin{bmatrix}
    0 \\
    0.8 \\
    0
  \end{bmatrix} +
  \begin{bmatrix}
    0 \\
    0.1 \\
    0
  \end{bmatrix} w_k
\end{equation}

where the Gaussian random variable $w_k$ has a mean of $0$ and a variance of
$1$. The observation \gls{model} is
\begin{equation}
  \mtx{y}_k =
  \begin{bmatrix}
    1 & 0 & 0 \\
    -1 & 0 & 1
  \end{bmatrix} \mtx{x}_k + \theta_k
\end{equation}

where the covariance matrix of Gaussian measurement noise $\theta$ is a
$2 \times 2$ matrix with both diagonals $10 cm^2$.

The \gls{state} vector is usually initialized using the first measurement or
two. The covariance matrix entries are assigned by calculating the covariance of
the expressions used when assigning the state vector. Let $k = 2$.
\begin{align}
  \mtx{Q} &= \begin{bmatrix}1\end{bmatrix} \\
  \mtx{R} &=
  \begin{bmatrix}
    10 & 0 \\
    0 & 10
  \end{bmatrix} \\
  \hat{\mtx{x}} &=
  \begin{bmatrix}
    \mtx{y}_{k,1} \\
    (\mtx{y}_{k,1} - \mtx{y}_{k-1,1})/dt \\
    \mtx{y}_{k,1} + \mtx{y}_{k,2}
  \end{bmatrix} \\
  \mtx{P} &=
  \begin{bmatrix}
    10 & 10/dt & 10 \\
    10/dt & 20/dt^2 & 10/dt \\
    10 & 10/dt & 20
  \end{bmatrix}
\end{align}

\subsubsection{Initial conditions}

To fill in the $\mtx{P}$ matrix, we calculate the covariance of each combination
of \gls{state} variables. The resulting value is a measure of how much those
variables are correlated. Due to how the covariance calculation works out, the
covariance between two variables is the sum of the variance of matching terms
which aren't constants multiplied by any constants the two have. If no terms
match, the variables are uncorrelated and the covariance is zero.

In $\mtx{P}_{11}$, the terms in $\mtx{x}_1$ correlate with itself. Therefore,
$\mtx{P}_{11}$ is $\mtx{x}_1$'s variance, or $\mtx{P}_{11} = 10$. For
$\mtx{P}_{21}$, One term correlates between $\mtx{x}_1$ and $\mtx{x}_2$, so
$\mtx{P}_{21} = \frac{10}{dt}$. The constants from each are simply multiplied
together. For $\mtx{P}_{22}$, both measurements are correlated, so the variances
add together. Therefore, $\mtx{P}_{22} = \frac{20}{dt^2}$. It continues in this
fashion until the matrix is filled up. Order doesn't matter for correlation, so
the matrix is symmetric.

\subsubsection{Selection of priors}

Choosing good priors is important for a well performing filter, even if little
information is known. This applies to both the measurement noise and the noise
\gls{model}. The act of giving a \gls{state} variable a large variance means you
know something about the \gls{system}. Namely, you aren't sure whether your
initial guess is close to the true \gls{state}. If you make a guess and specify
a small variance, you are telling the filter that you are very confident in your
guess. If that guess is incorrect, it will take the filter a long time to move
away from your guess to the true value.

\subsubsection{Covariance selection}

While one could assume no correlation between the \gls{state} variables and set
the covariance matrix entries to zero, this may not reflect reality. The Kalman
filter is still guarenteed to converge to the steady-state covariance after an
infinite time, but it will take longer than otherwise.

\subsubsection{Noise model selection}

We typically use a Gaussian distribution for the noise \gls{model} because the
sum of many independent random variables produces a normal distribution by the
central limit theorem. Kalman filters only require that the noise is zero-mean.
If the true value has an equal probability of being anywhere within a certain
range, use a uniform distribution instead. Each of these communicates
information regarding what you know about a system in addition to what you do
not.

\subsubsection{Process noise and measurement noise covariance selection}

Recall that the process noise covariance is $\mtx{Q}$ and the measurement noise
covariance is $\mtx{R}$. To tune the elements of these, it can be helpful to
take a collection of measurements, then run the Kalman filter on them offline to
evaluate its performance.

The diagonal elements of $\mtx{R}$ are the variances of each measurement, which
can be easily determined from the offline measurements. The diagonal elements of
$\mtx{Q}$ are the variances of each \gls{state}. They represent how much each
\gls{state} is expected to deviate from the \gls{model}.

Selecting $\mtx{Q}$ is more difficult. If the data is trusted too much over the
model, one risks overfitting the data. One should balance estimating any hidden
\glspl{state} sufficiently with actually filtering out the noise.

\subsubsection{Modeling other noise colors}

The Kalman filter assumes a \gls{model} with zero-mean white noise. If the
\gls{model} is incomplete in some way, whether it's missing dynamics or assumes
an incorrect noise \gls{model}, the residual
$\widetilde{\mtx{y}} = \mtx{y} - \mtx{C}\hat{\mtx{x}}$ over time will have
probability characteristics not indicative of white noise (e.g., it isn't
zero-mean).

To handle other colors of noise in a Kalman filter, define that color of noise
in terms of white noise and augment the \gls{model} with it.

\subsection{Simulation}
\label{subsec:filter_simulation}

Figure \ref{fig:filter_all} shows the \gls{state} estimates and measurements of
the Kalman filter over time. Figure \ref{fig:filter_robot_pos} shows the
position estimate and variance over time. Figure \ref{fig:filter_wall_pos} shows
the wall position estimate and variance over time. Notice how the variances
decrease over time as the filter gathers more measurements. This means that the
filter becomes more confident in its \gls{state} estimates.

The final precisions in estimating the position of the robot and the wall are
the square roots of the corresponding elements in the covariance matrix. That
is, $0.5188\,m$ and $0.4491\,m$ respectively. They are smaller than the
precision of the raw measurements, $\sqrt{10} = 3.1623\,m$. As expected,
combining the information from several measurements produces a better estimate
than any one measurement alone.
\begin{svg}{build/\chapterpath/kalman_filter_all}
  \caption{State estimates and measurements with Kalman filter}
  \label{fig:filter_all}
\end{svg}
\begin{svg}{build/\chapterpath/kalman_filter_robot_pos}
  \caption{Robot position estimate and variance with Kalman filter}
  \label{fig:filter_robot_pos}
\end{svg}
\begin{svg}{build/\chapterpath/kalman_filter_wall_pos}
  \caption{Wall position estimate and variance with Kalman filter}
  \label{fig:filter_wall_pos}
\end{svg}

\subsection{Kalman filter as Luenberger observer}
\index{state-space observers!Kalman filter!as Luenberger observer}

A Kalman filter can be represented as a Luenberger \gls{observer} by letting
$\mtx{L} = \mtx{A} \mtx{K}_k$ (see appendix \ref{sec:deriv_kalman_luenberger}).
The Luenberger observer has a constant observer gain matrix $\mtx{L}$, so the
steady-state Kalman gain is used to calculate it. We will demonstrate how to
find this shortly.

Kalman filter theory provides a way to place the poles of the Luenberger
observer optimally in the same way we placed the poles of the controller
optimally with LQR. The eigenvalues of the Kalman filter are
\begin{equation}
  \eig(\mtx{A}(\mtx{I} - \mtx{K}_k\mtx{C}))
\end{equation}

\subsubsection{Steady-state Kalman gain}

One may have noticed that the error covariance matrix can be updated
independently of the rest of the \gls{model}. The error covariance matrix tends
toward a steady-state value, and this matrix can be obtained via the discrete
algebraic Riccati equation. This can then be used to compute a steady-state
Kalman gain.

Snippet \ref{lst:kalman} computes the steady-state matrices for a Kalman
filter.
\begin{code}{Python}{build/frccontrol/frccontrol/kalmd.py}
  \caption{Steady-state Kalman gain and error covariance matrices calculation in
    Python}
  \label{lst:kalman}
\end{code}
