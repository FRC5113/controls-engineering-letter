\section{Kalman smoother}
\index{state-space observers!Kalman smoother}

The Kalman filter uses the data up to the current time to produce an optimal
estimate of the system \gls{state}. If data beyond the current time is
available, it can be ran through a Kalman smoother to produce a better estimate.
This is done by recording measurements, then applying the smoother to it
offline.

The Kalman smoother does a forward pass on the available data, then a backward
pass through the system dynamics so it takes into account the data before and
after the current time. This produces \gls{state} variances that are lower than
that of a Kalman filter.

\subsection{Derivations}
\index{state-space observers!Kalman smoother!derivations}

\subsection{State update equation}

Let $\hat{\mat{x}}(t)$ be the state estimate from the forward pass based on
samples $0$ to $t$ and $\hat{\mat{x}}_b(t)$ be the state estimate from the
backward pass based on samples $T$ to $t$.
\begin{equation}
  \hat{\mat{x}}(t|T) = \mat{A}\hat{\mat{x}}(t) + \mat{A}'\hat{\mat{x}}_b(t)
    \label{eq:ks_xhat}
\end{equation}

where $\mat{A}$ and $\mat{A}'$ are weighting factors.
\begin{equation*}
  \hat{\mat{x}}(t|T) = \mat{A}\mat{x}(t) + \mat{A}\widetilde{\mat{x}}(t) +
    \mat{A}'\mat{x}(t) + \mat{A}'\widetilde{\mat{x}}_b(t)
\end{equation*}

where $\widetilde{\mat{x}}(t)$ represents the error in the forward state
estimate and $\widetilde{\mat{x}}_b(t)$ represents the error in the backward
state estimate.
\begin{align*}
  \mat{x}(t) + \widetilde{\mat{x}}(t|T) &= \mat{A}\mat{x}(t) +
    \mat{A}\widetilde{\mat{x}}(t) + \mat{A}'\mat{x}(t) +
    \mat{A}'\widetilde{\mat{x}}_b(t) \\
  \widetilde{\mat{x}}(t|T) &= \mat{A}\mat{x}(t) - \mat{x}(t) +
    \mat{A}\widetilde{\mat{x}}(t) + \mat{A}'\mat{x}(t) +
    \mat{A}'\widetilde{\mat{x}}_b(t)
\end{align*}

Factor out $\mat{x}(t)$.
\begin{equation*}
  \widetilde{\mat{x}}(t|T) = (\mat{A} + \mat{A}' - \mat{I})\mat{x}(t) +
    \mat{A}\widetilde{\mat{x}}(t) + \mat{A}'\widetilde{\mat{x}}_b(t)
\end{equation*}

For unbiased filtering errors such as $\widetilde{\mat{x}}(t)$ and
$\widetilde{\mat{x}}_b(t)$, we want to have an unbiased smoothing error.
Therefore, we set $\mat{A} + \mat{A}' - \mat{I}$ to zero. This yields
$\mat{A}' = \mat{I} - \mat{A}$, so
\begin{equation*}
  \widetilde{\mat{x}}(t|T) = \mat{A}\widetilde{\mat{x}}(t) +
    (\mat{I} - \mat{A})\widetilde{\mat{x}}_b(t)
\end{equation*}

\subsection{Error covariance equation}

Next, find the error covariance.
\begin{equation}
  \mat{P}(t|T) = \mat{A}\mat{P}(t)\mat{A}^T +
    (\mat{I} - \mat{A})\mat{P}_b(t)(\mat{I} - \mat{A})^T
    \label{eq:ks_error_cov_a}
\end{equation}

Find the minimum of the trace of $\mat{P}(t|T)$ by taking the partial derivaitve
with respect to $\mat{A}$ and setting the result to $\mat{0}$ ($(t)$ has been
dropped from covariance matrices for clarity).
\begin{align}
  \mat{0} &= 2\mat{A}\mat{P} + 2(\mat{I} - \mat{A})\mat{P}_b(-\mat{I}) \nonumber
    \\
  \mat{0} &= 2\mat{A}\mat{P} - 2(\mat{I} - \mat{A})\mat{P}_b \nonumber \\
  \mat{0} &= \mat{A}\mat{P} - (\mat{I} - \mat{A})\mat{P}_b \nonumber \\
  \mat{0} &= \mat{A}\mat{P} - (\mat{P}_b - \mat{A}\mat{P}_b) \nonumber \\
  \mat{0} &= \mat{A}\mat{P} - \mat{P}_b + \mat{A}\mat{P}_b \nonumber \\
  \mat{0} &= \mat{A}(\mat{P} + \mat{P}_b) - \mat{P}_b \nonumber \\
  \mat{A}(\mat{P} + \mat{P}_b) &= \mat{P}_b \nonumber \\
  \mat{A} &= \mat{P}_b(\mat{P} + \mat{P}_b)^{-1} \label{eq:ks_a1}
\end{align}
\begin{align}
  \mat{I} - \mat{A} &= \mat{I} - \mat{P}_b(\mat{P} + \mat{P}_b)^{-1} \nonumber
    \\
  \mat{I} - \mat{A} &= (\mat{P} + \mat{P}_b)(\mat{P} + \mat{P}_b)^{-1} -
    \mat{P}_b(\mat{P} + \mat{P}_b)^{-1} \nonumber \\
  \mat{I} - \mat{A} &= (\mat{P} + \mat{P}_b - \mat{P}_b)
    (\mat{P} + \mat{P}_b)^{-1} \nonumber \\
  \mat{I} - \mat{A} &= \mat{P}(\mat{P} + \mat{P}_b)^{-1}
\end{align}
\begin{align*}
  \mat{A}^T &= (\mat{P}_b(\mat{P} + \mat{P}_b)^{-1})^T \\
  \mat{A}^T &= ((\mat{P} + \mat{P}_b)^{-1})^T\mat{P}_b^T \\
  \mat{A}^T &= ((\mat{P} + \mat{P}_b)^T)^{-1}\mat{P}_b^T \\
  \mat{A}^T &= (\mat{P}^T + \mat{P}_b^T)^{-1}\mat{P}_b^T
\end{align*}

Covariance matrices are symmetric, so
\begin{equation}
  \mat{A}^T = (\mat{P} + \mat{P}_b)^{-1}\mat{P}_b
\end{equation}
\begin{align*}
  (\mat{I} - \mat{A})^T &= (\mat{P}(\mat{P} + \mat{P}_b)^{-1})^T \\
  (\mat{I} - \mat{A})^T &= ((\mat{P} + \mat{P}_b)^{-1})^T\mat{P}^T \\
  (\mat{I} - \mat{A})^T &= ((\mat{P} + \mat{P}_b)^T)^{-1}\mat{P}^T \\
  (\mat{I} - \mat{A})^T &= (\mat{P}^T + \mat{P}_b^T)^{-1}\mat{P}^T
\end{align*}

Covariance matrices are symmetric, so
\begin{equation}
  (\mat{I} - \mat{A})^T = (\mat{P} + \mat{P}_b)^{-1}\mat{P} \label{eq:ks_a4}
\end{equation}

Now starting from equation \eqref{eq:ks_error_cov_a}, substitute in equations
\eqref{eq:ks_a1} through \eqref{eq:ks_a4}.
\begin{align*}
  \mat{P}(t|T) &=
    \mat{A}\mat{P}\mat{A}^T + (\mat{I} - \mat{A})\mat{P}_b(\mat{I} - \mat{A})^T
    \\
  \mat{P}(t|T) &=
    (\mat{P}_b(\mat{P} + \mat{P}_b)^{-1})\mat{P}
      ((\mat{P} + \mat{P}_b)^{-1}\mat{P}_b) +
    (\mat{P}(\mat{P} + \mat{P}_b)^{-1})\mat{P}_b
      ((\mat{P} + \mat{P}_b)^{-1}\mat{P}) \\
  \mat{P}(t|T) &=
    \mat{P}_b(\mat{P} + \mat{P}_b)^{-1}\mat{P}
    (\mat{P} + \mat{P}_b)^{-1}\mat{P}_b + \mat{P}
      (\mat{P} + \mat{P}_b)^{-1}\mat{P}_b
      (\mat{P} + \mat{P}_b)^{-1}\mat{P}
\end{align*}

Apply theorem \ref{thm:2mat_inv} to the right sides of each term to combine
them.
\begin{theorem}
  \label{thm:2mat_inv}

  $\mat{A}\mat{B} = (\mat{B}^{-1}\mat{A}^{-1})^{-1}$
\end{theorem}
\begin{align*}
  \mat{P}(t|T) &=
    \mat{P}_b(\mat{P} + \mat{P}_b)^{-1}\mat{P}
      (\mat{P}_b^{-1}(\mat{P} + \mat{P}_b))^{-1} +
    \mat{P}(\mat{P} + \mat{P}_b)^{-1}\mat{P}_b
      (\mat{P}^{-1}(\mat{P} + \mat{P}_b))^{-1} \\
  \mat{P}(t|T) &= \mat{P}_b(\mat{P} + \mat{P}_b)^{-1}\mat{P}
      (\mat{P}_b^{-1}\mat{P} + \mat{I})^{-1} +
    \mat{P}(\mat{P} + \mat{P}_b)^{-1}\mat{P}_b
      (\mat{I} + \mat{P}^{-1}\mat{P}_b)^{-1}
\end{align*}

Apply theorem \ref{thm:2mat_inv} to the right sides of each term again.
\begin{align*}
  \mat{P}(t|T) &=
    \mat{P}_b(\mat{P} + \mat{P}_b)^{-1}
      ((\mat{P}_b^{-1}\mat{P} + \mat{I})\mat{P}^{-1})^{-1} +
    \mat{P}(\mat{P} + \mat{P}_b)^{-1}
      ((\mat{I} + \mat{P}^{-1}\mat{P}_b)\mat{P}_b^{-1})^{-1} \\
  \mat{P}(t|T) &=
    \mat{P}_b(\mat{P} + \mat{P}_b)^{-1}(\mat{P}_b^{-1} + \mat{P}^{-1})^{-1} +
    \mat{P}(\mat{P} + \mat{P}_b)^{-1}(\mat{P}_b^{-1} + \mat{P}^{-1})^{-1} \\
  \mat{P}(t|T) &= \mat{P}_b(\mat{P} + \mat{P}_b)^{-1}
    (\mat{P}^{-1} + \mat{P}_b^{-1})^{-1} +
    \mat{P}(\mat{P} + \mat{P}_b)^{-1}(\mat{P}^{-1} + \mat{P}_b^{-1})^{-1}
\end{align*}

Factor out $(\mat{P}^{-1} + \mat{P}_b^{-1})^{-1}$ to the right.
\begin{equation*}
  \mat{P}(t|T) =
    (\mat{P}_b(\mat{P} + \mat{P}_b)^{-1} + \mat{P}(\mat{P} + \mat{P}_b)^{-1})
    (\mat{P}^{-1} + \mat{P}_b^{-1})^{-1}
\end{equation*}

Factor out $(\mat{P} + \mat{P}_b)^{-1}$ to the right.
\begin{align}
  \mat{P}(t|T) &=
    (\mat{P}_b + \mat{P})(\mat{P} + \mat{P}_b)^{-1}
    (\mat{P}^{-1} + \mat{P}_b^{-1})^{-1} \nonumber \\
  \mat{P}(t|T) &=
    (\mat{P} + \mat{P}_b)(\mat{P} + \mat{P}_b)^{-1}
    (\mat{P}^{-1} + \mat{P}_b^{-1})^{-1} \nonumber \\
  \mat{P}(t|T) &= I(\mat{P}^{-1} + \mat{P}_b^{-1})^{-1} \nonumber \\
  \mat{P}(t|T) &= (\mat{P}(t)^{-1} + \mat{P}_b(t)^{-1})^{-1}
    \label{eq:ks_error_cov}
\end{align}

\subsection{Optimal estimate}

Now find the optimal estimate $\hat{\mat{x}}(t|T)$ starting from equation
\eqref{eq:ks_xhat}.
\begin{align*}
  \hat{\mat{x}}(t|T) &= \mat{A}\hat{\mat{x}}(t) + \mat{A}'\hat{\mat{x}}_b(t) \\
  \hat{\mat{x}}(t|T) &= \mat{A}\hat{\mat{x}}(t) +
    (\mat{I} - \mat{A})\hat{\mat{x}}_b(t) \\
  \hat{\mat{x}}(t|T) &=
    \mat{P}_b(\mat{P} + \mat{P}_b)^{-1}\hat{\mat{x}}(t) +
    \mat{P}(\mat{P} + \mat{P}_b)^{-1}\hat{\mat{x}}_b(t) \\
  \hat{\mat{x}}(t|T) &=
    \mat{P}_b(\mat{P} + \mat{P}_b)^{-1}\mat{P}\mat{P}^{-1}\hat{\mat{x}}(t) +
    \mat{P}(\mat{P} + \mat{P}_b)^{-1}\mat{P}_b\mat{P}_b^{-1}\hat{\mat{x}}_b(t)
\end{align*}

Apply theorem \ref{thm:2mat_inv}.
\begin{align*}
  \hat{\mat{x}}(t|T) &=
    \mat{P}_b(\mat{P}^{-1}(\mat{P} + \mat{P}_b))^{-1}
      \mat{P}^{-1}\hat{\mat{x}}(t) +
    \mat{P}(\mat{P}_b^{-1}(\mat{P} + \mat{P}_b))^{-1}
      \mat{P}_b^{-1}\hat{\mat{x}}_b(t) \\
  \hat{\mat{x}}(t|T) &=
    \mat{P}_b(\mat{I} + \mat{P}^{-1}\mat{P}_b)^{-1}
      \mat{P}^{-1}\hat{\mat{x}}(t) +
    \mat{P}(\mat{P}_b^{-1}\mat{P} + \mat{I})^{-1}
      \mat{P}_b^{-1}\hat{\mat{x}}_b(t)
\end{align*}

Apply theorem \ref{thm:2mat_inv} again.
\begin{align*}
  \hat{\mat{x}}(t|T) &=
    ((\mat{I} + \mat{P}^{-1}\mat{P}_b)\mat{P}_b^{-1})^{-1}
      \mat{P}^{-1}\hat{\mat{x}}(t) +
    ((\mat{P}_b^{-1}\mat{P} + \mat{I})\mat{P}^{-1})^{-1}
      \mat{P}_b^{-1}\hat{\mat{x}}_b(t) \\
  \hat{\mat{x}}(t|T) &=
    (\mat{P}_b^{-1} + \mat{P}^{-1})^{-1}\mat{P}^{-1}\hat{\mat{x}}(t) +
    (\mat{P}_b^{-1} + \mat{P}^{-1})^{-1}\mat{P}_b^{-1}\hat{\mat{x}}_b(t)
\end{align*}

Factor out $(\mat{P}_b^{-1} + \mat{P}^{-1})^{-1}$ to the left.
\begin{equation*}
  \hat{\mat{x}}(t|T) =
    (\mat{P}_b^{-1} + \mat{P}^{-1})^{-1}
    (\mat{P}^{-1}\hat{\mat{x}}(t) + \mat{P}_b^{-1}\hat{\mat{x}}_b(t))
\end{equation*}

Substitute in equation \eqref{eq:ks_error_cov}.
\begin{align}
  \hat{\mat{x}}(t|T) &= \mat{P}(t|T)
    (\mat{P}^{-1}\hat{\mat{x}}(t) + \mat{P}_b^{-1}\hat{\mat{x}}_b(t)) \nonumber
    \\
  \hat{\mat{x}}(t|T) &= \mat{P}(t|T)
    (\mat{P}^{-1}(t)\hat{\mat{x}}(t) + \mat{P}_b^{-1}(t)\hat{\mat{x}}_b(t))
\end{align}

\subsection{Predict and update equations}

One first does a forward pass with the typical Kalman filter equations and
stores the results. Then one can use the Rauch-Tung-Striebel (RTS) algorithm to
do the backward pass (see theorem \ref{thm:kalman_smoother}).

Theorem \ref{thm:kalman_smoother} shows the predict and and update steps for the
forward and backward passes for a Kalman smoother at the $k^{th}$ timestep.

\index{state-space observers!Kalman smoother!equations}
\begin{theorem}[Kalman smoother]
  \label{thm:kalman_smoother}
  \begin{align}
    \text{Forward predict step} \nonumber \\
    \hat{\mat{x}}_{k+1}^- &= \mat{A}\hat{\mat{x}}_k + \mat{B} \mat{u}_k \\
    \mat{P}_{k+1}^- &= \mat{A} \mat{P}_k^- \mat{A}^T +
      \mat{\Gamma}\mat{Q}\mat{\Gamma}^T \\
    \text{Forward update step} \nonumber \\
    \mat{K}_{k+1} &=
      \mat{P}_{k+1}^- \mat{C}^T (\mat{C}\mat{P}_{k+1}^- \mat{C}^T +
      \mat{R})^{-1} \\
    \hat{\mat{x}}_{k+1}^+ &=
      \hat{\mat{x}}_{k+1}^- + \mat{K}_{k+1}(\mat{y}_{k+1} -
      \mat{C} \hat{\mat{x}}_{k+1}^- - \mat{D}\mat{u}_{k+1}) \\
    \mat{P}_{k+1}^+ &= (\mat{I} - \mat{K}_{k+1}\mat{C})\mat{P}_{k+1}^- \\
    \text{Backward update step} \nonumber \\
    \mat{K}_k &= \mat{P}_k^+ \mat{A}_k^T (\mat{P}_{k+1}^-)^{-1} \\
    \hat{\mat{x}}_{k|N} &= \hat{\mat{x}}_k^+ +
      \mat{K}_k(\hat{\mat{x}}_{k+1|N} - \hat{\mat{x}}_{k+1}^-) \\
    \mat{P}_{k|N} &=
      \mat{P}_k^+ + \mat{K}_k(\mat{P}_{k+1|N} - \mat{P}_{k+1}^-)\mat{K}_k^T \\
    \text{Backward initial conditions} \nonumber \\
    \hat{\mat{x}}_{N|N} &= \hat{\mat{x}}_N^+ \\
    \mat{P}_{N|N} &= \mat{P}_N^+
  \end{align}
\end{theorem}

\subsection{Example}

We will modify the robot model so that instead of a velocity of $0.8 cm/s$ with
random noise, the velocity is modeled as a random walk from the current
velocity.
\begin{equation}
  \mat{x}_k =
  \begin{bmatrix}
    x_k \\
    v_k \\
    x_k^w
  \end{bmatrix}
\end{equation}
\begin{equation}
  \mat{x}_{k+1} =
  \begin{bmatrix}
    1 & 1 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1
  \end{bmatrix} \mat{x}_k +
  \begin{bmatrix}
    0 \\
    0.1 \\
    0
  \end{bmatrix} w_k
\end{equation}

We will use the same observation model as before.

Using the same data from subsection \ref{subsec:filter_simulation}, figures
\ref{fig:smoother_robot_pos}, \ref{fig:smoother_robot_vel}, and
\ref{fig:smoother_wall_pos} show the improved \gls{state} estimates and figure
\ref{fig:smoother_robot_pos_variance} shows the improved robot position
covariance with a Kalman smoother.

Notice how the wall position produced by the smoother is a constant. This is
because that \gls{state} has no dynamics, so the final estimate from the Kalman
filter is already the best estimate.
\begin{svg}{build/\chapterpath/kalman_smoother_robot_pos}
  \caption{Robot position with Kalman smoother}
  \label{fig:smoother_robot_pos}
\end{svg}
\begin{svg}{build/\chapterpath/kalman_smoother_robot_vel}
  \caption{Robot velocity with Kalman smoother}
  \label{fig:smoother_robot_vel}
\end{svg}
\begin{svg}{build/\chapterpath/kalman_smoother_wall_pos}
  \caption{Wall position with Kalman smoother}
  \label{fig:smoother_wall_pos}
\end{svg}
\begin{svg}{build/\chapterpath/kalman_smoother_robot_pos_variance}
  \caption{Robot position variance with Kalman smoother}
  \label{fig:smoother_robot_pos_variance}
\end{svg}

See Roger Labbe's book \textit{Kalman and Bayesian Filters in Python} for more
on
smoothing\footnote{\url{https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/13-Smoothing.ipynb}}.
