\section{Linear stochastic systems}
\index{stochastic!linear systems}
\index{state-space observers!Kalman filter!derivations}

Given the following stochastic system
\begin{align*}
  \mtx{x}_{k+1} &= \mtx{A}\mtx{x}_k + \mtx{B}\mtx{u}_k +
    \mtx{\Gamma}\mtx{w}_k \\
  \mtx{y}_k &= \mtx{C}\mtx{x}_k + \mtx{D}\mtx{u}_k + \mtx{v}_k
\end{align*}

where $\mtx{w}_k$ is the process noise and $\mtx{v}_k$ is the measurement noise,
\index{stochastic!process noise} \index{stochastic!measurement noise}
\begin{align*}
  E[\mtx{w}_k] &= 0 \\
  E[\mtx{w}_k\mtx{w}_k^T] &= \mtx{Q}_k \\
  E[\mtx{v}_k] &= 0 \\
  E[\mtx{v}_k\mtx{v}_k^T] &= \mtx{R}_k
\end{align*}

where $\mtx{Q}_k$ is the process noise covariance matrix and $\mtx{R}_k$ is the
measurement noise covariance matrix. We assume the noise samples are
independent, so $E[\mtx{w}_k\mtx{w}_j^T] = 0$ and $E[\mtx{v}_k\mtx{v}_k^T] = 0$
where $k \neq j$. Furthermore, process noise samples are independent from
measurement noise samples.

We'll compute the expectation of these equations and their covariance matrices,
which we'll use later for deriving the Kalman filter.

\subsection{State vector expectation evolution}

First, we will compute how the expectation of the \gls{system} \gls{state}
evolves.
\begin{align*}
  E[\mtx{x}_{k+1}] &= E[\mtx{A}\mtx{x}_k + \mtx{B}\mtx{u}_k +
    \mtx{\Gamma}\mtx{w}_k] \\
  E[\mtx{x}_{k+1}] &= E[\mtx{A}\mtx{x}_k] + E[\mtx{B}\mtx{u}_k] +
    E[\mtx{\Gamma}\mtx{w}_k] \\
  E[\mtx{x}_{k+1}] &= \mtx{A}E[\mtx{x}_k] + \mtx{B}E[\mtx{u}_k] +
    \mtx{\Gamma}E[\mtx{w}_k] \\
  E[\mtx{x}_{k+1}] &= \mtx{A}E[\mtx{x}_k] + \mtx{B}\mtx{u}_k + 0 \\
  \mtxbar{x}_{k+1} &= \mtx{A}\mtxbar{x}_k + \mtx{B}\mtx{u}_k \\
\end{align*}

\subsection{Error covariance matrix evolution}

Now, we will use this to compute how the \gls{error} covariance matrix $\mtx{P}$
evolves.
\begin{align*}
  \mtx{x}_{k+1} - \mtxbar{x}_{k+1} &= \mtx{A}\mtx{x}_k +
    \mtx{B}\mtx{u}_k + \mtx{\Gamma}\mtx{w}_k - (\mtx{A}\mtxbar{x}_k -
    \mtx{B}\mtx{u}_k) \\
  \mtx{x}_{k+1} - \mtxbar{x}_{k+1} &=
    \mtx{A}(\mtx{x}_k - \mtxbar{x}_k) + \mtx{\Gamma}\mtx{w}_k
\end{align*}
\begin{equation*}
  E[(\mtx{x}_{k+1} - \mtxbar{x}_{k+1})(\mtx{x}_{k+1} - \mtxbar{x}_{k+1})^T] =
    E[(\mtx{A}(\mtx{x}_k - \mtxbar{x}_k) + \mtx{\Gamma}\mtx{w}_k)
      (\mtx{A}(\mtx{x}_k - \mtxbar{x}_k) + \mtx{\Gamma}\mtx{w}_k)^T]
\end{equation*}
\begin{align*}
  \mtx{P}_{k+1} =~&
    E[(\mtx{A}(\mtx{x}_k - \mtxbar{x}_k) + \mtx{\Gamma}\mtx{w}_k)
      (\mtx{A}(\mtx{x}_k - \mtxbar{x}_k) + \mtx{\Gamma}\mtx{w}_k)^T] \\
  \mtx{P}_{k+1} =~&
    E[(\mtx{A}(\mtx{x}_k - \mtxbar{x}_k)(\mtx{x}_k - \mtxbar{x}_k)^T
      \mtx{A}^T] +
    E[\mtx{A}(\mtx{x}_k - \mtxbar{x}_k)\mtx{w}_k^T\mtx{\Gamma}^T] + \\
    &E[\mtx{\Gamma}\mtx{w}_k(\mtx{x}_k - \mtxbar{x}_k)^T\mtx{A}^T] +
    E[\mtx{\Gamma}\mtx{w}_k\mtx{w}_k^T\mtx{\Gamma}^T] \\
  \mtx{P}_{k+1} =~&
    \mtx{A}E[(\mtx{x}_k - \mtxbar{x}_k)(\mtx{x}_k - \mtxbar{x}_k)^T]
    \mtx{A}^T +
    \mtx{A}E[(\mtx{x}_k - \mtxbar{x}_k)\mtx{w}_k^T]\mtx{\Gamma}^T + \\
    &\mtx{\Gamma} E[\mtx{w}_k(\mtx{x}_k - \mtxbar{x}_k)^T]\mtx{A}^T +
    \mtx{\Gamma} E[\mtx{w}_k\mtx{w}_k^T]\mtx{\Gamma}^T \\
  \mtx{P}_{k+1} =~& \mtx{A}\mtx{P}_k\mtx{A}^T +
    \mtx{A}E[(\mtx{x}_k - \mtxbar{x}_k)\mtx{w}_k^T]\mtx{\Gamma}^T +
    \mtx{\Gamma} E[\mtx{w}_k(\mtx{x}_k - \mtxbar{x}_k)^T]\mtx{A}^T +
    \mtx{\Gamma}\mtx{Q}_k\mtx{\Gamma}_k^T
\end{align*}

Since the error and noise are independent, the cross terms are zero.
\begin{align*}
  \mtx{P}_{k+1} =~& \mtx{A}\mtx{P}_k\mtx{A}^T +
    \mtx{A}\underbrace{
      E[(\mtx{x}_k - \mtxbar{x}_k)\mtx{w}_k^T]}_\mtx{0}\mtx{\Gamma}^T +
    \mtx{\Gamma}\underbrace{
      E[\mtx{w}_k(\mtx{x}_k - \mtxbar{x}_k)^T]}_\mtx{0}\mtx{A}^T +
    \mtx{\Gamma}\mtx{Q}_k\mtx{\Gamma}_k^T \\
  \mtx{P}_{k+1} =~& \mtx{A}\mtx{P}_k\mtx{A}^T +
    \mtx{\Gamma}\mtx{Q}_k\mtx{\Gamma}^T
\end{align*}

\subsection{Measurement vector expectation}

Next, we will compute the expectation of the \gls{output} $\mtx{y}$.
\begin{align*}
  E[\mtx{y}_k] &= E[\mtx{C}\mtx{x}_k + \mtx{D}\mtx{u}_k + \mtx{v}_k] \\
  E[\mtx{y}_k] &= \mtx{C}E[\mtx{x}_k] + \mtx{D}\mtx{u}_k + 0 \\
  \mtxbar{y}_k &= \mtx{C}\mtxbar{x}_k + \mtx{D}\mtx{u}_k
\end{align*}

\subsection{Measurement covariance matrix}

Now, we will use this to compute how the measurement covariance matrix
$\mtx{S}$ evolves.
\begin{align*}
  \mtx{y}_k - \mtxbar{y}_k &= \mtx{C}\mtx{x}_k + \mtx{D}\mtx{u}_k + \mtx{v}_k -
    (\mtx{C}\mtxbar{x}_k + \mtx{D}\mtx{u}_k) \\
  \mtx{y}_k - \mtxbar{y}_k &= \mtx{C}(\mtx{x}_k - \mtxbar{x}_k) + \mtx{v}_k
\end{align*}
\begin{align*}
  E[(\mtx{y}_k - \mtxbar{y}_k)(\mtx{y}_k - \mtxbar{y}_k)^T] =~&
    E[(\mtx{C}(\mtx{x}_k - \mtxbar{x}_k) + \mtx{v}_k)
      (\mtx{C}(\mtx{x}_k - \mtxbar{x}_k) + \mtx{v}_k)^T] \\
  \mtx{S}_k =~&
    E[(\mtx{C}(\mtx{x}_k - \mtxbar{x}_k) + \mtx{v}_k)
      (\mtx{C}(\mtx{x}_k - \mtxbar{x}_k) + \mtx{v}_k)^T] \\
  \mtx{S}_k =~&
    E[(\mtx{C}(\mtx{x}_k - \mtxbar{x}_k) + \mtx{v}_k)
      ((\mtx{x}_k - \mtxbar{x}_k)^T\mtx{C}^T + \mtx{v}_k^T)] \\
  \mtx{S}_k =~&
    E[(\mtx{C}(\mtx{x}_k - \mtxbar{x}_k)(\mtx{x}_k - \mtxbar{x}_k)^T
      \mtx{C}^T] +
    E[\mtx{C}(\mtx{x}_k - \mtxbar{x}_k)\mtx{v}_k^T] + \\
    &E[\mtx{v}_k(\mtx{x}_k - \mtxbar{x}_k)^T\mtx{C}^T] +
    E[\mtx{v}_k\mtx{v}_k^T] \\
  \mtx{S}_k =~&
    \mtx{C}E[(\mtx{x}_k - \mtxbar{x}_k)(\mtx{x}_k - \mtxbar{x}_k)^T]
    \mtx{C}^T + \mtx{C}E[(\mtx{x}_k - \mtxbar{x}_k)\mtx{v}_k^T] + \\
    &E[\mtx{v}_k(\mtx{x}_k - \mtxbar{x}_k)^T]\mtx{C}^T +
    E[\mtx{v}_k\mtx{v}_k^T] \\
  \mtx{S}_k =~& \mtx{C}\mtx{P}_k\mtx{C}^T +
    \mtx{C}E[(\mtx{x}_k - \mtxbar{x}_k)\mtx{v}_k^T] +
    E[\mtx{v}_k(\mtx{x}_k - \mtxbar{x}_k)^T]\mtx{C}^T +
    \mtx{R}_k
\end{align*}

Since the error and noise are independent, the cross terms are zero.
\begin{align*}
  \mtx{S}_k =~& \mtx{C}\mtx{P}_k\mtx{C}^T +
    \mtx{C}\underbrace{E[(\mtx{x}_k - \mtxbar{x}_k)\mtx{v}_k^T]}_\mtx{0} +
    \underbrace{
      E[\mtx{v}_k(\mtx{x}_k - \mtxbar{x}_k)^T]}_\mtx{0}\mtx{C}^T + \mtx{R}_k \\
  \mtx{S}_k =~& \mtx{C}\mtx{P}_k\mtx{C}^T + \mtx{R}_k
\end{align*}
