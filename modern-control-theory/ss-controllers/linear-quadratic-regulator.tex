\section{Linear-quadratic regulator} \label{sec:lqr}
\index{controller design!linear-quadratic regulator}
\index{optimal control!linear-quadratic regulator}

\subsection{The intuition}

We can demonstrate the basic idea behind the linear-quadratic regulator with the
following flywheel model:
\begin{equation*}
  \dot{x} = ax + bu
\end{equation*}

where $a$ is a negative constant representing the back-EMF of the motor, $x$ is
the angular velocity, $b$ is a positive constant that maps the input voltage to
some change in angular velocity (angular acceleration), $u$ is the voltage
applied to the motor, and $\dot{x}$ is the angular acceleration.

If the angular velocity starts from zero and we apply a positive voltage, we'd
see the motor spin up to some constant speed following an exponential decay,
then stay at that speed. If we throw in the control law $u = k(r - x)$, we can
make the system converge to a desired state $r$ through proportional feedback.
In what manner can we pick the constant $k$ that balances getting to the target
angular velocity quickly with getting there efficiently? (That is, quickly, but
without oscillation or excessive voltage)

We can solve this problem with something called the linear-quadratic regulator.
We'll define the following cost function that includes the states and inputs:
\begin{equation*}
  J = \int_0^\infty (Q(r - x)^2 + Ru^2) \,dt
\end{equation*}

We want to minimize this while obeying the constraint that the system follow our
flywheel dynamics $\dot{x} = ax + bu$.

The cost is the sum of the squares of the error and the input for all time. If
the controller gain $k$ we pick in the control law $u = k(r - x)$ is stable, the
error $r - x$ and the input $u$ will both go to zero and give us a finite cost.
$Q$ and $R$ let us decide how much the error and input contribute to the cost
(we will require that $Q \geq 0$ and $R > 0$ for reasons that will be clear
shortly\footnote{Lets consider the boundary conditions on the weights $Q$ and
$R$. If we set $Q$ to zero, error doesn't contribute to the cost, so the optimal
solution is to not move. This minimizes the sum of the inputs over time. If we
let $R$ be zero, the input doesn't contribute to the cost, so infinite inputs
are allowed as they minimize the sum of the errors over time. This isn't useful,
so we require that the input be penalized with a nonzero $R$.}). Penalizing
error more will make the controller more aggressive, while penalizing the input
more will make the controller less aggressive. We want to pick a $k$ that
minimizes the cost.

There's a common trick for finding the value of a variable that minimizes a
function of that variable. We'll take the derivative (the slope) of the cost
function with respect to the input $u$, set the derivative to zero, then solve
for $u$. When the slope is zero, the function is at a minimum or maximum. Now,
the cost function we picked is quadratic. All the terms are strictly positive on
account of the squared variables and nonnegative weights, so our cost is
strictly positive and the quadratic function is concave up. The $u$ we found is
therefore a minimum.

The actual process of solving for $u$ is mathematically intensive and outside
the scope of this explanation (appendix \ref{ch:deriv_lqr} references a
derivation for those curious). The rest of this section will describe the more
general form of the linear-quadratic regulator and how to use it.

\subsection{The mathematical definition}

Instead of placing the poles of a closed-loop \gls{system} manually, the
linear-quadratic regulator (LQR) places the poles for us based on acceptable
relative \gls{error} and \gls{control effort} costs. This method of controller
design uses a quadratic function for the cost-to-go defined as the sum of the
\gls{error} and \gls{control effort} over time for the linear \gls{system}
$\dot{\mtx{x}} = \mtx{A}\mtx{x} + \mtx{B}\mtx{u}$.
\begin{equation*}
  J = \int\limits_0^\infty \left(\mtx{x}^T\mtx{Q}\mtx{x} +
    \mtx{u}^T\mtx{R}\mtx{u}\right) dt
\end{equation*}

where $J$ represents a trade-off between \gls{state} excursion and
\gls{control effort} with the weighting factors $\mtx{Q}$ and $\mtx{R}$. LQR
finds a \gls{control law} $\mtx{u}$ that minimizes the cost function. $\mtx{Q}$
and $\mtx{R}$ slide the cost along a Pareto boundary between state tracking and
\gls{control effort} (see figure \ref{fig:pareto_boundary}). Pareto optimality
for this problem means that an improvement in state \gls{tracking} cannot be
obtained without using more \gls{control effort} to do so. Also, a reduction in
\gls{control effort} cannot be obtained without sacrificing state \gls{tracking}
performance. Pole placement, on the other hand, will have a cost anywhere on,
above, or to the right of the Pareto boundary (no cost can be inside the
boundary).
\begin{svg}{build/\chapterpath/pareto_boundary}
  \caption{Pareto boundary for LQR}
  \label{fig:pareto_boundary}
\end{svg}

The minimum of LQR's cost function is found by setting the derivative of the
cost function to zero and solving for the \gls{control law} $\mtx{u}$. However,
matrix calculus is used instead of normal calculus to take the derivative.

The feedback \gls{control law} that minimizes $J$ is shown in theorem
\ref{thm:linear-quadratic_regulator}.
\begin{theorem}[Linear-quadratic regulator]
  \label{thm:linear-quadratic_regulator}

  \begin{align}
    \min_{\mtx{u}} &\int\limits_0^\infty \left(\mtx{x}^T\mtx{Q}\mtx{x} +
      \mtx{u}^T\mtx{R}\mtx{u}\right) dt \nonumber \\
    \text{subject to } &\dot{\mtx{x}} = \mtx{A}\mtx{x} + \mtx{B}\mtx{u}
  \end{align}

  If the \gls{system} is controllable, the optimal control policy $\mtx{u}^*$
  that drives all the \glspl{state} to zero is $-\mtx{K}\mtx{x}$. To converge to
  nonzero \glspl{state}, a \gls{reference} vector $\mtx{r}$ can be added to the
  \gls{state} $\mtx{x}$.

  \begin{equation}
    \mtx{u} = \mtx{K}(\mtx{r} - \mtx{x})
  \end{equation}
\end{theorem}
\index{controller design!linear-quadratic regulator!definition}
\index{optimal control!linear-quadratic regulator!definition}

This means that optimal control can be achieved with simply a set of
proportional gains on all the \glspl{state}. To use the \gls{control law}, we
need knowledge of the full \gls{state} of the \gls{system}. That means we either
have to measure all our \glspl{state} directly or estimate those we do not
measure.

See appendix \ref{ch:deriv_lqr} for how $\mtx{K}$ is calculated. If the result
is finite, the controller is guaranteed to be stable and
\glslink{robustness}{robust} with a \gls{phase margin} of 60 degrees
\cite{bib:lqr_phase_margin}.
\begin{remark}
  LQR design's $\mtx{Q}$ and $\mtx{R}$ matrices don't need \gls{discretization},
  but the $\mtx{K}$ calculated for continuous time and discrete time
  \glspl{system} will be different. The discrete time gains approach the
  continuous time gains as the sample period tends to zero.
\end{remark}

\subsection{Bryson's rule}
\index{controller design!linear-quadratic regulator!Bryson's rule}
\index{optimal control!linear-quadratic regulator!Bryson's rule}

The next obvious question is what values to choose for $\mtx{Q}$ and $\mtx{R}$.
While this can be more of an art than a science, Bryson's rule provides a good
starting point. With Bryson's rule, the diagonals of the $\mtx{Q}$ and $\mtx{R}$
matrices are chosen based on the maximum acceptable value for each \gls{state}
and actuator. The nondiagonal elements are zero. The balance between $\mtx{Q}$
and $\mtx{R}$ can be slid along the Pareto boundary using a weighting factor
$\rho$.
\begin{equation*}
  J = \int\limits_0^\infty \left(\rho \left[
    \left(\frac{x_1}{x_{1,max}}\right)^2 + \ldots +
    \left(\frac{x_m}{x_{m,max}}\right)^2\right] + \left[
    \left(\frac{u_1}{u_{1,max}}\right)^2 + \ldots +
    \left(\frac{u_n}{u_{n,max}}\right)^2\right]\right) dt
\end{equation*}
\begin{equation*}
  \begin{array}{cc}
    \mtx{Q} = \begin{bmatrix}
      \frac{\rho}{x_{1,max}^2} & 0 & \ldots & 0 \\
      0 & \frac{\rho}{x_{2,max}^2} & & \vdots \\
      \vdots & & \ddots & 0 \\
      0 & \ldots & 0 & \frac{\rho}{x_{m,max}^2}
    \end{bmatrix} &
    \mtx{R} = \begin{bmatrix}
      \frac{1}{u_{1,max}^2} & 0 & \ldots & 0 \\
      0 & \frac{1}{u_{2,max}^2} & & \vdots \\
      \vdots & & \ddots & 0 \\
      0 & \ldots & 0 & \frac{1}{u_{n,max}^2}
    \end{bmatrix}
  \end{array}
\end{equation*}

Small values of $\rho$ penalize \gls{control effort} while large values of
$\rho$ penalize \gls{state} excursions. Large values would be chosen in
applications like fighter jets where performance is necessary. Spacecrafts would
use small values to conserve their limited fuel supply.

\subsection{Pole placement vs LQR}

This example uses the following second-order \gls{model} for a CIM motor (a DC
brushed motor).
\begin{align*}
  \begin{array}{cccc}
    \mtx{A} = \begin{bmatrix}
      -\frac{b}{J} & \frac{K_t}{J} \\
      -\frac{K_e}{L} & -\frac{R}{L}
    \end{bmatrix} &
    \mtx{B} = \begin{bmatrix}
      0 \\
      \frac{1}{L}
    \end{bmatrix} &
    \mtx{C} = \begin{bmatrix}
      1 & 0
    \end{bmatrix} &
    \mtx{D} = \begin{bmatrix}
      0
    \end{bmatrix}
  \end{array}
\end{align*}

Figure \ref{fig:case_study_pp_lqr} shows the response using discrete
poles\footnote{See section \ref{sec:s-plane_to_z-plane} for pole mappings of
discrete systems (inside unit circle is stable). The pole mappings mentioned so
far (LHP is stable) only apply to continuous systems.} placed at $(0.1, 0)$ and
$(0.9, 0)$ and LQR with the following cost matrices.
\begin{align*}
  \begin{array}{cc}
    \mtx{Q} = \begin{bmatrix}
      \frac{1}{20^2} & 0 \\
      0 & \frac{1}{40^2}
    \end{bmatrix} &
    \mtx{R} = \begin{bmatrix}
      \frac{1}{12^2}
    \end{bmatrix}
  \end{array}
\end{align*}
\begin{svg}{build/\chapterpath/case_study_pp_lqr}
  \caption{Second-order CIM motor response with pole placement and LQR}
  \label{fig:case_study_pp_lqr}
\end{svg}

LQR selected poles at $(0.593, 0)$ and $(0.955, 0)$. Notice with pole placement
that as the current pole moves left, the \gls{control effort} becomes more
aggressive.
