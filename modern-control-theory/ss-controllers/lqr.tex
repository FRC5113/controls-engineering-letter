\section{LQR} \label{sec:lqr}
\index{Controller design!LQR}
\index{Optimal control!LQR}

Instead of placing the poles of a closed-loop \gls{system} manually, LQR design
places the poles for us based on acceptable \gls{error} and \gls{control effort}
constraints. ``LQR" stands for ``Linear-Quadratic
\glslink{regulator}{Regulator}". This method of controller design uses a
quadratic function for the cost-to-go defined as the sum of the \gls{error} and
\gls{control effort} over time for the linear \gls{system}
$\dot{\mtx{x}} = \mtx{A}\mtx{x} + \mtx{B}\mtx{u}$.

\begin{equation*}
  J = \int\limits_0^\infty \left(\mtx{x}^T\mtx{Q}\mtx{x} +
    \mtx{u}^T\mtx{R}\mtx{u}\right) dt
\end{equation*}

where $J$ represents a tradeoff between \gls{state} excursion and
\gls{control effort} with the weighting factors $\mtx{Q}$ and $\mtx{R}$. LQR
finds a \gls{control law} $\mtx{u}$ that minimizes the cost function. $\mtx{Q}$
and $\mtx{R}$ slide the cost along a Pareto boundary between state tracking and
\gls{control effort} (see figure \ref{fig:pareto_boundary}). Pareto optimality
for this problem means that an improvement in state \gls{tracking} cannot be
obtained without using more \gls{control effort} to do so. Also, a reduction in
\gls{control effort} cannot be obtained without sacrificing state \gls{tracking}
performance. Pole placement, on the other hand, will have a cost anywhere on,
above, or to the right of the Pareto boundary (no cost can be inside the
boundary).

\begin{svg}{build/code/pareto_boundary}
  \caption{Pareto boundary for LQR}
  \label{fig:pareto_boundary}
\end{svg}

The minimum of LQR's cost function is found by setting the derivative of the
cost function to zero and solving for the \gls{control law} $\mtx{u}$. However,
matrix calculus is used instead of normal calculus to take the derivative.

The feedback \gls{control law} that minimizes $J$, which we'll call the
``optimal control law", is shown in theorem \ref{thm:optimal_control_law}.

\begin{theorem}[Optimal control law]
  \label{thm:optimal_control_law}

  \begin{equation}
    \mtx{u} = -\mtx{K}\mtx{x}
  \end{equation}
\end{theorem}
\index{Controller design!LQR!optimal control law}
\index{Optimal control!LQR!optimal control law}

This means that optimal control can be achieved with simply a set of
proportional gains on all the \glspl{state}. This \gls{control law} will make
all \glspl{state} converge to zero assuming the \gls{system} is controllable. To
converge to nonzero \glspl{state}, a \gls{reference} vector $\mtx{r}$ can be
added to the \gls{state} $\mtx{x}$.

\begin{theorem}[Optimal control law with nonzero reference]
  \begin{equation}
    \mtx{u} = \mtx{K}(\mtx{r} - \mtx{x})
  \end{equation}
\end{theorem}

To use the \gls{control law}, we need knowledge of the full \gls{state} of the
\gls{system}. That means we either have to measure all our \glspl{state}
directly or estimate those we do not measure.

See appendix \ref{sec:deriv_lqr} for how $\mtx{K}$ is calculated in Python. If
the result is finite, the controller is guaranteed to be stable and
\glslink{robustness}{robust} with a \gls{phase margin} of 60 degrees
\cite{bib:lqr_phase_margin}.

\begin{remark}
  LQR design's $\mtx{Q}$ and $\mtx{R}$ matrices don't need \gls{discretization},
  but the $\mtx{K}$ calculated for continuous time and discrete time
  \glspl{system} will be different.
\end{remark}

\subsection{Bryson's rule}
\index{Controller design!LQR!Bryson's rule}
\index{Optimal control!LQR!Bryson's rule}

The next obvious question is what values to choose for $\mtx{Q}$ and $\mtx{R}$.
With Bryson's rule, the diagonals of the $\mtx{Q}$ and $\mtx{R}$ matrices are
chosen based on the maximum acceptable value for each \gls{state} and actuator.
The nondiagonal elements are zero. The balance between $\mtx{Q}$ and $\mtx{R}$
can be slid along the Pareto boundary using a weighting factor $\rho$.

\begin{equation*}
  J = \int\limits_0^\infty \left(\rho \left[
    \left(\frac{x_1}{x_{1,max}}\right)^2 + \ldots +
    \left(\frac{x_n}{x_{n,max}}\right)^2\right] + \left[
    \left(\frac{u_1}{u_{1,max}}\right)^2 + \ldots +
    \left(\frac{u_n}{u_{n,max}}\right)^2\right]\right) dt
\end{equation*}

\begin{equation*}
  \begin{array}{cc}
    \mtx{Q} = \begin{bmatrix}
      \frac{\rho}{x_{1,max}^2} & 0 & \ldots & 0 \\
      0 & \frac{\rho}{x_{2,max}^2} & & \vdots \\
      \vdots & & \ddots & 0 \\
      0 & \ldots & 0 & \frac{\rho}{x_{n,max}^2}
    \end{bmatrix} &
    \mtx{R} = \begin{bmatrix}
      \frac{1}{u_{1,max}^2} & 0 & \ldots & 0 \\
      0 & \frac{1}{u_{2,max}^2} & & \vdots \\
      \vdots & & \ddots & 0 \\
      0 & \ldots & 0 & \frac{1}{u_{n,max}^2}
    \end{bmatrix}
  \end{array}
\end{equation*}

Small values of $\rho$ penalize \gls{control effort} while large values of
$\rho$ penalize \gls{state} excursions. Large values would be chosen in
applications like fighter jets where performance is necessary. Spacecrafts would
use small values to conserve their limited fuel supply.
