\section{Matrix exponential}
\index{discretization!matrix exponential}
\begin{remark}
  Watch the ``How (and why) to raise e to the power of a matrix" video from
  3Blue1Brown's \textit{Essence of linear algebra} series (27 minutes)
  \cite{bib:linalg_matrix_exp} for a visual introduction to the matrix
  exponential.
\end{remark}
\begin{definition}[Matrix exponential]
  Let $\mat{X}$ be an $n \times n$ matrix. The exponential of $\mat{X}$ denoted
  by $e^{\mat{X}}$ is the $n \times n$ matrix given by the following power
  series.
  \begin{equation}
    e^{\mat{X}} = \sum_{k=0}^\infty \frac{1}{k!} \mat{X}^k \label{eq:mat_exp}
  \end{equation}

  where $\mat{X}^0$ is defined to be the identity matrix $\mat{I}$ with the same
  dimensions as $\mat{X}$.
\end{definition}

To understand why the matrix exponential is used in the \gls{discretization}
process, consider the scalar differential equation $\dot{x} = ax$. The solution
to this type of differential equation uses an exponential.
\begin{align*}
  \dot{x} &= ax \\
  \frac{dx}{dt} &= ax(t) \\
  dx &= ax(t) \,dt \\
  \frac{1}{x(t)} \,dx &= a \,dt \\
  \int_0^t \frac{1}{x(t)} \,dx &= \int_0^t a \,dt \\
  \ln(x(t)) \rvert_0^t &= at \rvert_0^t \\
  \ln(x(t)) - \ln(x(0)) &= at - a \cdot 0 \\
  \ln(x(t)) - \ln(x_0) &= at \\
  \ln\left(\frac{x(t)}{x_0}\right) &= at \\
  \frac{x(t)}{x_0} &= e^{at} \\
  x(t) &= e^{at} x_0
\end{align*}

This solution generalizes via the matrix exponential to the set of differential
equations $\dot{\mat{x}} = \mat{A}\mat{x} + \mat{B}\mat{u}$ we use to describe
\glspl{system} (see section \ref{sec:deriv_zoh_ss} for a complete derivation).
\begin{equation*}
  \mat{x}(t) = e^{\mat{A}t} \mat{x}_0 +
    \mat{A}^{-1}(e^{\mat{A}t} - \mat{I})\mat{B} \mat{u}
\end{equation*}

where $\mat{x}_0$ contains the initial conditions and $\mat{u}$ is the constant
input from time $0$ to $t$. If the initial \gls{state} is the current system
\gls{state}, then we can describe the \gls{system}'s \gls{state} over time as
\begin{align*}
  \mat{x}_{k+1} &= e^{\mat{A}T} \mat{x}_k +
    \mat{A}^{-1}(e^{\mat{A}T} - \mat{I})\mat{B} \mat{u}_k
  \intertext{or more compactly,}
  \mat{x}_{k+1} &= \mat{A}_d\mat{x}_k + \mat{B}_d\mat{u}_k
\end{align*}

where $T$ is the time between samples $\mat{x}_k$ and $\mat{x}_{k+1}$. Theorem
\ref{thm:zoh_ss} has more efficient ways to compute $\mat{A}_d$ and $\mat{B}_d$.
