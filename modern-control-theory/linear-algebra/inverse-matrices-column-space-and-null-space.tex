\section{Inverse matrices, column space, and null space}

\begin{remark}
  See the corresponding \textit{Essence of linear algebra} video for a more
  visual presentation (12 minutes)
  \cite{bib:linalg_inverse_matrices_column_space_and_null_space}.
\end{remark}

As you can probably tell by now, the bulk of this chapter is on understanding
matrix and vector operations through that more visual lens of linear
transformations. This section is no exception, describing the concepts of
inverse matrices, columns space, rank, and null space through that lens. A fair
warning though: we're not going to talk about the methods for actually computing
these things, and some would argue that that's pretty important. There are a lot
of very good resources for learning those methods outside of this chapter.
Keywords: ``Gaussian elimination" and ``row echelon form". Most of the value
that we actually have to add here is on the intuition half. Plus, in practice,
we usually use software to compute these things for us anyway.

\subsection{Linear systems of equations}
\index{Matrices!linear systems}

First, a few words on the usefulness of linear algebra. By now, you already have
a hint for how it's used in describing the manipulation of space, which is
useful for computer graphics and robotics. However, one of the main reasons that
linear algebra is more broadly applicable, and required for just about any
technical discipline, is that it lets us solve certain systems of equations.
When we say ``system of equations", we mean there is a list of variables, things
you don't know, and a list of equations relating them. For example,

\begin{align*}
  6x - 3y + 2z &= 7 \\
  x + 2y + 5z &= 0 \\
  2x - 8y - z &= -2
\end{align*}

is a system of equations with the unknowns $x$, $y$, and $z$.

In a lot of situations, those equations can get very complicated, but, if you're
lucky, they might take on a certain special form. Within each equation, the only
thing happening to each variable is that it's scaled by some constant, and the
only thing happening to each of those scaled variables is that they're added to
each other, so no exponents or fancy functions, or multiplying two variables
together.

The typical way to organize this sort of special system of equations is to throw
all the variables on the left and put any lingering constants on the right. It's
also nice to vertically line up the common variables, and to do that, you might
need to throw in some zero coefficients whenever the variable doesn't show up in
one of the equations.

\begin{align*}
  2x + 5y + 3z &= -3 \\
  4x + 0y + 8z &= 0 \\
  1x + 3y + 0z &= 2
\end{align*}

This is called a ``linear system of equations". You might notice that this looks
a lot like matrix-vector multiplication. In fact, you can package all of the
equations together into a single vector equation, where you have the matrix
containing all the constant coefficients, a vector containing all the constant
coefficients, and a vector containing all the variables. Their matrix-vector
product equals some different constant vector.

\begin{equation*}
  \begin{bmatrix}
    2 & 5 & 3 \\
    4 & 0 & 8 \\
    1 & 3 & 0
  \end{bmatrix}
  \begin{bmatrix}
    x \\
    y \\
    z
  \end{bmatrix} =
  \begin{bmatrix}
    -3 \\
    0 \\
    2
  \end{bmatrix}
\end{equation*}

Let's name that constant matrix $\mtx{A}$, denote the vector holding the
variables with $\mtx{x}$, and call the constant vector on the right-hand side
$\mtx{v}$. This is more than just a notational trick to get our system of
equations written on one line. It sheds light on a pretty cool geometric
interpretation for the problem.

\begin{equation*}
  \mtx{A}\mtx{x} = \mtx{v}
\end{equation*}

The matrix $\mtx{A}$ corresponds with some linear transformation, so solving
$\mtx{A}\mtx{x} = \mtx{v}$ means we're looking for a vector $\mtx{x}$ which,
after applying the transformation $\mtx{A}$, lands on $\mtx{v}$.

Think about what's happening here for a moment. You can hold in your head this
really complicated idea of multiple variables all intermingling with each other
just by thinking about compressing or morphing space and trying to determine
which vector lands on another.

To start simple, let's say you have a system with two equations and two
unknowns. This means the matrix $\mtx{A}$ is a $2 \times 2$ matrix, and
$\mtx{v}$ and $\mtx{x}$ are each two-dimensional vectors.

\begin{align*}
  2x + 2y &= -4 \\
  1x + 3y &= -1 \\
  \begin{bmatrix}
    2 & 2 \\
    1 & 3
  \end{bmatrix}
  \begin{bmatrix}
    x \\
    y
  \end{bmatrix} &=
  \begin{bmatrix}
    -4 \\
    -1
  \end{bmatrix}
\end{align*}

\subsection{Inverse}
\index{Matrices!inverse}

How we think about the solutions to this equation depends on whether the
transformation associated with $\mtx{A}$ compresses all of space into a lower
dimension, like a line or a point, or if it leaves everything spanning the full
two dimensions where it started. In the language of the last section, we
subdivide into the case where $\mtx{A}$ has zero determinant and the case where
$\mtx{A}$ has nonzero determinant.

Let's start with the most likely case where the determinant is nonzero, meaning
space does not get compressed into a zero area region. In this case, there will
always be one and only one vector that lands on $\mtx{v}$, and you can find it
by playing the transformation in reverse. Following where $\mtx{v}$ goes as we
undo the transformation, you'll find the vector $\mtx{x}$ such that $\mtx{A}$
times $\mtx{x}$ equals $\mtx{v}$.

When you play the transformation in reverse, it actually corresponds to a
separate linear transformation, commonly called the ``inverse of $\mtx{A}$"
denoted $\mtx{A}^{-1}$.

\begin{equation*}
  \mtx{A}^{-1} =
  \begin{bmatrix}
    3 & 1 \\
    0 & 2
  \end{bmatrix}^{-1}
\end{equation*}

For example, if $\mtx{A}$ was a counterclockwise rotation by $90 \degree$, then
the inverse of $\mtx{A}$ would be a clockwise rotation by $90 \degree$.

\begin{equation*}
  \begin{array}{cc}
  \mtx{A} =
  \begin{bmatrix}
    0 & -1 \\
    1 & 0
  \end{bmatrix} &
  \mtx{A}^{-1} =
  \begin{bmatrix}
    0 & 1 \\
    -1 & 0
  \end{bmatrix}
  \end{array}
\end{equation*}

If $\mtx{A}$ was a rightward shear that pushes $\hat{j}$ one unit to the right,
the inverse of $\mtx{A}$ would be a leftward shear that pushes $\hat{j}$ one
unit to the left.

In general, $\mtx{A}^{-1}$ is the unique transformation with the property that
if you first apply $\mtx{A}$, then follow it with the transformation
$\mtx{A}^{-1}$, you end up back where you started. Applying one transformation
after another is captured algebraically with matrix multiplication, so the core
property of this transformation $\mtx{A}^{-1}$ is that $\mtx{A}^{-1}\mtx{A}$
equals the matrix that corresponds to doing nothing.

The transformation that does nothing is called the ``identity transformation".
It leaves $\hat{i}$ and $\hat{j}$ each where they are, unmoved, so its columns
are $(1, 0)$ and $(0, 1)$.

\begin{equation*}
  \mtx{A}^{-1}\mtx{A} =
  \begin{bmatrix}
    1 & 0 \\
    0 & 1
  \end{bmatrix}
\end{equation*}

Once you find this inverse, which in practice, you do with a computer, you can
solve your equation by multipling this inverse matrix by $\mtx{v}$.

\begin{align*}
  \mtx{A}\mtx{x} &= \mtx{v} \\
  \mtx{A}^{-1}\mtx{A}\mtx{x} &= \mtx{A}^{-1}\mtx{v} \\
  \mtx{x} &= \mtx{A}^{-1}\mtx{v}
\end{align*}

Again, what this means geometrically is that you're playing the transformation
in reverse and following $\mtx{v}$. This nonzero determinant case, which for a
random choice of matrix is by far the most likely one, corresponds with the idea
that if you have two unknowns and two equations, it's almost certainly the case
that there's a single, unique solution.

This idea also makes sense in higher dimensions when the number of equations
equals the number of unknowns. Again, the system of equations can be translated
to the geometric interpretation where you have some transformation, $\mtx{A}$,
some vector $\mtx{v}$, and you're looking for the vector $\mtx{x}$ that lands on
$\mtx{v}$. As long as the transformation $\mtx{A}$ doesn't compress all of
space into a lower dimension, meaning, its determinant is nonzero, there will be
an inverse transformation, $\mtx{A}^{-1}$ , with the property that if you first
do $\mtx{A}$, then you do $\mtx{A}^{-1}$, it's the same as doing nothing. To
solve your equation, you just have to multiply that reverse transformation
matrix by the vector $\mtx{v}$.

When the determinant is zero and the transformation associated with this system
of equations compresses space into a smaller dimension, there is no inverse. You
cannot uncompress a line to turn it into a plane. At least, that's not something
that a function can do. That would require transforming each individual vector
into a whole line full of vectors, but functions can only take a single input to
a single output.

Similarly, for three equations and three unknowns, there will be no inverse if
the corresponding transformation compresses 3D space onto the plane, or even if
it compresses it onto a line, or a point. Those all correspond to a determinant
of zero since any region is compressed into something with zero volume.

It's still possible that a solution exists even when there is no inverse. It's
just that when your transformation compresses space onto, say, a line, you have
to be lucky enough that the vector $\mtx{v}$ exists somewhere on that line.

\subsection{Rank and column space}
\index{Matrices!rank}

You might notice that some of these zero determinant cases feel a lot more
restrictive than others. Given a $3 \times 3$ matrix, for example, it seems a
lot harder for a solution to exist when it compresses space onto a line compared
to when it compresses space onto a plane even though both of those have zero
determinant. We have some language that's more specific than just saying ``zero
determinant". When the output of a transformation is a line, meaning it's
one-dimensional, we say the transformation has a \textit{rank} of one. If all
the vectors land on some two-dimensional plane, we say the transformation has a
rank of two. The word ``rank" means the number of dimensions in the output of a
transformation.

For instance, in the case of $2 \times 2$ matrices, the highest possible rank is
$2$. It means the basis vectors continue to span the full two dimensions of
space, and the determinant is nonzero. For $3 \times 3$ matrices, rank $2$ means
that we've collapsed, but not as much as we would have collapsed for a rank 1
situation. If a 3D transformation has a nonzero determinant and its output fills
all of 3D space, it has a rank of $3$.

This set of all possible outputs for your matrix, whether it's a line, a plane,
3D space, whatever, is called the \textit{column space} of your matrix. You can
probably guess where that name comes from. The columns of your matrix tell you
where the basis vectors land, and the span of those transformed basis vectors
gives you all possible outputs. In other words, the column space is the span of
the columns of your matrix, so a more precise definition of rank would be that
it's the number of dimensions in the column space. When this rank is as high as
it can be, meaning it equals the number of columns, we call the matrix ``full
rank".

\subsection{Null space}

Notice, the zero vector will always be included in the column space since linear
transformations must keep the origin fixed in place. For a full rank
transformation, the only vector that lands at the origin is the zero vector
itself, but for matrices that aren't full rank, which compress to a smaller
dimension, you can have a whole bunch of vectors that land on zero. If a 2D
transformation compresses space onto a line, for example, there is a separate
line in a different direction full of vectors that get compressed onto the
origin. If a 3D transformation compresses space onto a plane, there's also a
full line of vectors that land on the origin. If a 3D transformation compresses
all the space onto a line, then there's a whole plane full of vectors that land
on the origin.

This set of vectors that lands on the origin is called the \textit{null space}
or the \textit{kernel} of your matrix. It's the space of all vectors that become
null in the sense that they land on the zero vector. In terms of the linear
system of equations $\mtx{A}\mtx{x} = \mtx{v}$, when $\mtx{v}$ happens to be the
zero vector, the null space gives you all the possible solutions to the
equation.

\subsection{Closing remarks}

That's a high-level overview of how to think about linear systems of equations
geometrically. Each system has some kind of linear transformation associated
with it, and when that transformation has an inverse, you can use that inverse
to solve your system. Otherwise, the idea of column space lets us understand
when a solution even exists, and the idea of a null space helps us understand
what the set of all possible solutions can look like.

Again, there's a lot not covered here, most notably how to compute these things.
We also had to limit the scope to examples where the number of equations equals
the number of unknowns. The goal here is not to try to teach everything: it's
that you come away with a strong intuition for inverse matrices, column space,
and null space, and that those intuitions make any future learning that you do
more fruitful.
