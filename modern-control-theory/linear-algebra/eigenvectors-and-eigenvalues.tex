\section{Eigenvectors and eigenvalues}

\begin{remark}
  See the corresponding \textit{Essence of linear algebra} video for a more
  visual presentation (17 minutes)
  \cite{bib:linalg_eigenvectors_and_eigenvalues}.
\end{remark}

\subsection{What is an eigenvector?}

To start, consider some linear transformation in two dimensions that moves the
basis vector $\hat{i}$ to the coordinates $(3, 0)$ and $\hat{j}$ to $(1, 2)$, so
it's represented with a matrix whose columns are $(3, 0)$ and $(1, 2)$.

\begin{equation*}
  \begin{bmatrix}
    3 & 1 \\
    0 & 2
  \end{bmatrix}
\end{equation*}

Focus in on what it does to one particular vector and think about the span of
that vector, the line passing through its origin and its tip. Most vectors are
going to get knocked off their span during the transformation, but some special
vectors do remain on their own span meaning the effect that the matrix has on
such a vector is just to stretch it or compress it like a scalar.

For this specific example, the basis vector $\hat{i}$ is one such special
vector. The span of $\hat{i}$ is the x-axis, and from the first column of the
matrix, we can see that $\hat{i}$ moves over to three times itself still on that
x-axis. What's more, due to the way linear transformations work, any other
vector on the x-axis is also just stretched by a factor of $3$, and hence,
remains on its own span.

A slightly sneakier vector that remains on its own span during this
transformation is $(-1, 1)$. It ends up getting stretched by a factor of $2$.
Again, linearity is going to imply that any other vector on the diagonal line
spanned by this vector is just going to get stretched out by a factor of $2$.

For this transformation, those are all the vectors with this special property of
staying on their span. Those on the x-axis get stretched out by a factor of $3$
and those on the diagonal line get stretched out by a factor of $2$. Any other
vector is going to get rotated somewhat during the transformation and knocked
off the line that it spans. As you might have guessed by now, these special
vectors are called the \textit{eigenvectors} of the transformation, and each
eigenvector has associated with it an \textit{eigenvalue}, which is just the
factor by which it's stretched or compressed during the transformation.

Of course, there's nothing special about stretching vs compressing or the fact
that these eigenvalues happen to be positive. In another example, you could have
an eigenvector with eigenvalue $-\frac{1}{2}$, meaning that the vector gets
flipped and compressed by a factor of $\frac{1}{2}$.

\begin{equation*}
  \begin{bmatrix}
    0.5 & -1 \\
    -1 & 0.5
  \end{bmatrix}
\end{equation*}

The important part here is that it stays on the line that it spans out without
getting rotated off of it.

\subsection{Eigenvectors in 3D rotation}

For a glimpse of why this might be a useful thing to think about, consider some
three-dimensional rotation. If you can find an eigenvector for that rotation, a
vector that remains on its own span, you have found the axis of rotation. It's
much easier to think about a 3D rotation in terms of some axis of rotation and
an angle by which it's rotating rather than thinking about the full $3 \times 3$
matrix associated with that transformation. In this case, by the way, the
corresponding eigenvalue would have to be $1$ since rotations never stretch or
compress anything, so the length of the vector would remain the same.

\subsection{Finding eigenvalues}
\index{Matrices!eigenvalues}

The following pattern shows up a lot in linear algebra. With any linear
transformation described by a matrix, you could understand what it's doing by
reading off the columns of this matrix as the landing spots for basis vectors,
but often a better way to get at the heart of what the linear transformation
actually does, less dependent on your particular coordinate system, is to find
the eigenvectors and eigenvalues.

I won't cover the full details on methods for computing eigenvectors and
eigenvalues here, but I'll try to give an overview of the computational ideas
that are most important for a conceptual understanding. Symbolically, an
eigenvector look like the following

\begin{equation*}
  \mtx{A}\mtx{v} = \lambda \mtx{v}
\end{equation*}

$\mtx{A}$ is the matrix representing some transformation, $\mtx{v}$ is the
eigenvector, and $\lambda$ is a number, namely the corresponding eigenvalue.
This expression is saying that the matrix-vector product $\mtx{A}\mtx{v}$ gives
the same result as just scaling the eigenvector $\mtx{v}$ by some value
$\lambda$. Finding the eigenvectors and their eigenvalues of the matrix
$\mtx{A}$ involves finding the values of $\mtx{v}$ and $\lambda$ that make this
expression true. It's awkward to work with at first because that left-hand side
represents matrix-vector multiplication, but the right-hand side is
scalar-vector multiplication. Let's rewrite the right-hand side as some kind of
matrix-vector multiplication using a matrix which has the effect of scaling any
vector by a factor of $\lambda$. The columns of such a matrix will represent
what happens to each basis vector, and each basis vector is simply multiplied by
$\lambda$, so this matrix will have the number $\lambda$ down the diagonal and
zeroes everywhere else.

\begin{equation*}
  \begin{bmatrix}
    \lambda & 0 & 0 \\
    0 & \lambda & 0 \\
    0 & 0 & \lambda \\
  \end{bmatrix}
\end{equation*}

The common way to write this is to factor out $\lambda$ and write it as
$\lambda\mtx{I}$ where $\mtx{I}$ is the identity matrix with ones down the
diagonal.

\begin{equation*}
  \mtx{A}\mtx{v} = (\lambda\mtx{I}) \mtx{v}
\end{equation*}

With both sides looking like matrix-vector multiplication, we can subtract off
that right-hand side and factor out $\mtx{v}$.

\begin{align*}
  \mtx{A}\mtx{v} - (\lambda\mtx{I}) \mtx{v} &= \mtx{0} \\
  (\mtx{A} - \lambda\mtx{I})\mtx{v} &= \mtx{0}
\end{align*}

We now have a new matrix $\mtx{A} - \lambda\mtx{I}$, and we're looking for a
vector $\mtx{v}$ such that this new matrix times $\mtx{v}$ gives the zero
vector. This will always be true if $\mtx{v}$ itself is the zero vector, but
that's boring. We want a nonzero eigenvector. The only way it's possible for the
product of a matrix with a nonzero vector to become zero is if the
transformation associated with that matrix compresses space into a lower
dimension. That compression corresponds to a zero determinant for the matrix.

\begin{equation*}
  det(\mtx{A} - \lambda\mtx{I}) = 0
\end{equation*}

To be concrete, let's say your matrix $\mtx{A}$ has columns $(2, 1)$ and
$(2, 3)$, and think about subtracting off a variable amount $\lambda$.

\begin{equation*}
  det\left(\begin{bmatrix}
    2 - \lambda & 2 \\
    1 & 3 - \lambda
  \end{bmatrix}\right) = 0
\end{equation*}

The goal is to find a value of $\lambda$ that will make this determinant zero
meaning the tweaked transformation compresses space into a lower dimension. In
this case, that value is $\lambda = 1$. Of course, if we had chosen some other
matrix, the eigenvalue might not necessarily be $1$.

This is kind of a lot, but let's unravel what this is saying. When
$\lambda = 1$, the matrix $\mtx{A} - \lambda\mtx{I}$ compresses space onto a
line. That means there's a nonzero vector $\mtx{v}$ such that
$(\mtx{A} - \lambda\mtx{I})\mtx{v}$ equals the zero vector.

\begin{equation*}
  (\mtx{A} - \lambda\mtx{I})\mtx{v} = \mtx{0}
\end{equation*}

Remember, we care about that because it means
$\mtx{A}\mtx{v} = \lambda\mtx{v}$, which you can read off as saying that the
vector $\mtx{v}$ is an eigenvector of $\mtx{A}$ staying on its own span during
the transformation $\mtx{A}$. For the following example

\begin{equation*}
  \begin{bmatrix}
    2 & 2 \\
    1 & 3
  \end{bmatrix}
  \mtx{v} = 1\mtx{v}
\end{equation*}

the corresponding eigenvalue is $1$, so $\mtx{v}$ would actually just stay fixed
in place.

To summarize our line of reasoning:

\begin{align*}
  \mtx{A}\mtx{v} &= \lambda\mtx{v} \\
  \mtx{A}\mtx{v} - \lambda\mtx{I}\mtx{v} &= \mtx{0} \\
  (\mtx{A} - \lambda\mtx{I})\mtx{v} &= \mtx{0} \\
  det(\mtx{A} - \lambda\mtx{I}) &= \mtx{0}
\end{align*}

To see this in action, let's visit the example from the start with a matrix
whose columns are $(3, 0)$ and $(1, 2)$. To determine if a value $\lambda$ is an
eigenvalue, subtract it from the diagonals of this matrix and compute the
determinant.

\begin{align*}
  \begin{bmatrix}
    3 & 1 \\
    0 & 2
  \end{bmatrix} & \\
  \begin{bmatrix}
    3 - \lambda & 1 \\
    0 & 2 - \lambda
  \end{bmatrix} & \\
  det\left(\begin{bmatrix}
    3 - \lambda & 1 \\
    0 & 2 - \lambda
  \end{bmatrix}\right) &=
  (3 - \lambda)(2 - \lambda) - 1 \cdot 0 \\
  &= (3 - \lambda)(2 - \lambda)
\end{align*}

We get a certain quadratic polynomial in $\lambda$. Since $\lambda$ can only be
an eigenvalue if this determinant happens to be zero, you can conclude that the
only possible eigenvalues are $\lambda = 2$ and $\lambda = 3$.

To determine what the eigenvectors are that actually have one of these
eigenvalues, say $\lambda = 2$, plug in that value of $\lambda$ to the matrix
and then solve for which vectors this diagonally altered matrix sends to zero.

\begin{equation*}
  \begin{bmatrix}
    3 - 2 & 1 \\
    0 & 2 - 2
  \end{bmatrix}
  \begin{bmatrix}
    x \\
    y
  \end{bmatrix} =
  \begin{bmatrix}
    0 \\
    0
  \end{bmatrix}
\end{equation*}

If you computed this the way you would any other linear system, you'd see that
the solutions are all the vectors on the diagonal line spanned by $(-1, 1)$.
This corresponds to the fact that the unaltered matrix has the effect of
stretching all those vectors by a factor of $2$.

\subsection{Transformations with no eigenvectors}

A 2D transformation doesn't have to have eigenvectors. For example, consider a
rotation by $90 \degree$.

\begin{equation*}
  \begin{bmatrix}
    0 & -1 \\
    1 & 0
  \end{bmatrix}
\end{equation*}

This doesn't have any eigenvectors since it rotates every vector off its own
span. If you actually tried computing the eigenvalues of a rotation like this,
notice what happens.

\begin{align*}
  \begin{bmatrix}
    0 & -1 \\
    1 & 0
  \end{bmatrix} & \\
  \begin{bmatrix}
    - \lambda & -1 \\
    1 & -\lambda
  \end{bmatrix} & \\
  det\left(\begin{bmatrix}
    -\lambda & -1 \\
    1 & -\lambda
  \end{bmatrix}\right) &=
  (-\lambda)(-\lambda) - (-1)(1) \\
  &= \lambda^2 + 1 = 0
\end{align*}

The only roots of that polynomial are the imaginary numbers $i$ and $-i$. The
fact that there are no real number solutions indicates that there are no
eigenvectors.

\begin{remark}
  Interestingly though, the fact that multiplication by $i$ in the complex plane
  looks like a $90 \degree$ rotation is related to the fact that $i$ is an
  eigenvalue of this transformation of 2D real vectors. The specifics of this
  are out of scope, but note that eigenvalues which are complex numbers
  generally correspond to some kind of rotation in the transformation.
\end{remark}

\subsection{Repeated eigenvalues}

Another interesting example is a shear which fixes $\hat{i}$ in place and moves
$\hat{j}$ over by $1$.

\begin{equation*}
  \begin{bmatrix}
    1 & 1 \\
    0 & 1
  \end{bmatrix}
\end{equation*}

All the vectors on the x-axis are eigenvectors with eigenvalue $1$. In fact,
these are the only eigenvectors.

\begin{align*}
  det\left(\begin{bmatrix}
    1 - \lambda & 1 \\
    0 & 1 - \lambda
  \end{bmatrix}\right) &= (1 - \lambda)(1 - \lambda) = 0 \\
  (1 - \lambda)^2 &= 0
\end{align*}

The only root of this expression is $\lambda = 1$.

\subsection{Transformations with larger eigenvector spans}

Keep in mind it's also possible to have just one eigenvalue but with more than
just a line full of eigenvectors. A simple example is a matrix that scales
everything by $2$.

\begin{equation*}
  \begin{bmatrix}
    2 & 0 \\
    0 & 2
  \end{bmatrix}
\end{equation*}

The only eigenvalue is $2$, but every vector in the plane gets to be an
eigenvector with that eigenvalue.
